{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExplainMyXray v2 — MedGemma-4B + PadChest (RTX 4080 Laptop)\n",
    "\n",
    "**Optimized for RTX 4080 Laptop 12 GB VRAM** (compute capability 8.9 → bfloat16).\n",
    "\n",
    "### Architecture\n",
    "| Spec | Value |\n",
    "|------|-------|\n",
    "| Base Model | google/medgemma-4b-it (SigLIP encoder + Gemma 3 decoder) |\n",
    "| Fine-tuning | QLoRA NF4, LoRA r=64, alpha=128, all-linear + lm_head |\n",
    "| Precision | **bf16** (RTX 4080 supports bfloat16 natively) |\n",
    "| Dataset | BIMCV PadChest — 160K+ CXR, 174 findings, 104 locations |\n",
    "| Image size | 512x512 (padded to square, LANCZOS, CLAHE) |\n",
    "| Data access | Google Drive for Desktop (streams on-demand) or local |\n",
    "\n",
    "### Preprocessing Pipeline (from analysis of 200+ PadChest images)\n",
    "1. 16-bit I;16 -> 8-bit (percentile 1st/99th)\n",
    "2. Auto-crop dark edges (33% of images have artifacts)\n",
    "3. Pad to square (aspect ratios vary 0.8-1.22)\n",
    "4. LANCZOS resize to 512x512\n",
    "5. CLAHE (local contrast, clip=2.0)\n",
    "6. Sharpen 1.2x\n",
    "7. Convert to RGB (for SigLIP encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1 — Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 1: Install Dependencies (RTX 4080 compatible)\n",
    "# ============================================================\n",
    "import subprocess, sys, os, pathlib, platform\n",
    "\n",
    "packages = [\n",
    "    \"transformers>=4.52.0\",    # MedGemma support\n",
    "    \"trl>=0.17.0\",             # SFTTrainer\n",
    "    \"peft>=0.15.0\",            # LoRA/QLoRA\n",
    "    \"accelerate>=1.5.0\",       # device_map\n",
    "    \"bitsandbytes>=0.44.0\",    # 4-bit quantization\n",
    "    \"datasets>=3.5.0\",         # HuggingFace datasets\n",
    "    \"evaluate\",                # Metrics\n",
    "    \"scikit-learn\",            # Sklearn metrics\n",
    "    \"Pillow>=10.0\",            # Image loading\n",
    "    \"gdown\",                   # Google Drive downloads\n",
    "    \"opencv-python\",           # CLAHE (with GUI for desktop)\n",
    "    \"tensorboard\",             # Training logs\n",
    "]\n",
    "\n",
    "for pkg in packages:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg],\n",
    "                          stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "import torch, transformers, trl, peft\n",
    "print(f'torch={torch.__version__}, transformers={transformers.__version__}, '\n",
    "      f'trl={trl.__version__}, peft={peft.__version__}')\n",
    "print(f'CUDA: {torch.cuda.is_available()}, GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\"}')\n",
    "print(\"Dependencies installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 2: Auto-Detect Google Drive for Desktop (data source)\n",
    "# ============================================================\n",
    "# PadChest data can come from:\n",
    "#   1. Google Drive for Desktop (recommended — streams on-demand)\n",
    "#   2. Local download (needs ~300 GB)\n",
    "#   3. Partial download via gdown\n",
    "# ============================================================\n",
    "\n",
    "def _scan_gdrive_desktop():\n",
    "    \"\"\"Find PadChest on Google Drive for Desktop (Windows/macOS/Linux).\"\"\"\n",
    "    system = platform.system()\n",
    "    if system == 'Windows':\n",
    "        for letter in 'GDEFHIJKLMNOPQRSTUVWXYZ':\n",
    "            for name in ['Google Drive/My Drive/Padchest',\n",
    "                         'My Drive/Padchest',\n",
    "                         'GoogleDrive/My Drive/Padchest']:\n",
    "                candidate = f'{letter}:/{name}'\n",
    "                if os.path.isdir(candidate): return candidate\n",
    "    elif system == 'Darwin':\n",
    "        cloud = pathlib.Path.home() / 'Library' / 'CloudStorage'\n",
    "        if cloud.exists():\n",
    "            for folder in sorted(cloud.iterdir()):\n",
    "                if folder.name.startswith('GoogleDrive'):\n",
    "                    c = folder / 'My Drive' / 'Padchest'\n",
    "                    if c.is_dir(): return str(c)\n",
    "        legacy = '/Volumes/GoogleDrive/My Drive/Padchest'\n",
    "        if os.path.isdir(legacy): return legacy\n",
    "    elif system == 'Linux':\n",
    "        home = pathlib.Path.home()\n",
    "        for cand in [\n",
    "            home / 'Google Drive' / 'My Drive' / 'Padchest',\n",
    "            home / 'google-drive' / 'My Drive' / 'Padchest',\n",
    "            home / 'gdrive' / 'My Drive' / 'Padchest',\n",
    "            pathlib.Path('/mnt/google-drive/My Drive/Padchest'),\n",
    "        ]:\n",
    "            if cand.is_dir(): return str(cand)\n",
    "    # Check Colab\n",
    "    colab = '/content/drive/MyDrive/Padchest'\n",
    "    if os.path.isdir(colab): return colab\n",
    "    return None\n",
    "\n",
    "GDRIVE_PADCHEST = _scan_gdrive_desktop()\n",
    "if GDRIVE_PADCHEST:\n",
    "    print(f'Google Drive detected: {GDRIVE_PADCHEST}')\n",
    "    print('Data streams on-demand — no local download needed.')\n",
    "else:\n",
    "    print('Google Drive for Desktop not found.')\n",
    "    print('Options: 1) Install GDrive Desktop, 2) Set paths manually in Config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 3: Imports\n",
    "# ============================================================\n",
    "import os, ast, random, warnings, gc, time\n",
    "from pathlib import Path\n",
    "from typing import Any, Optional, Tuple\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import Rectangle\n",
    "from PIL import Image, ImageFilter, ImageEnhance, ImageOps\n",
    "import cv2\n",
    "\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    AutoModelForImageTextToText,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"transformers\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*PaliGemmaProcessor.*\")\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}, VRAM: {torch.cuda.get_device_properties(0).total_mem/1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 4: HuggingFace Authentication\n",
    "# ============================================================\n",
    "from huggingface_hub import login\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "if not hf_token:\n",
    "    try:\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        hf_token = UserSecretsClient().get_secret(\"HF_TOKEN\")\n",
    "    except: pass\n",
    "if not hf_token:\n",
    "    from huggingface_hub import notebook_login\n",
    "    notebook_login()\n",
    "else:\n",
    "    login(token=hf_token)\n",
    "    print(\"Logged in to HuggingFace.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 5: GPU Configuration — RTX 4080 (bf16 native)\n",
    "# ============================================================\n",
    "if not torch.cuda.is_available(): raise RuntimeError('No GPU!')\n",
    "\n",
    "cc = torch.cuda.get_device_capability(0)\n",
    "USE_BF16 = cc[0] >= 8  # RTX 4080 = cc 8.9 → bf16 OK\n",
    "COMPUTE_DTYPE = torch.bfloat16 if USE_BF16 else torch.float16\n",
    "\n",
    "print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "print(f'CC: {cc[0]}.{cc[1]}, Precision: {\"bf16\" if USE_BF16 else \"fp16\"}')\n",
    "print(f'VRAM: {torch.cuda.get_device_properties(0).total_mem/1024**3:.1f} GB')\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2 — Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 6: Master Configuration — RTX 4080 12 GB VRAM\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    model_id: str = \"google/medgemma-4b-it\"\n",
    "    use_full_padchest: bool = True\n",
    "\n",
    "    # ---- Paths (auto-detected from Google Drive or manual) ----\n",
    "    padchest_csv: str = \"\"\n",
    "    padchest_images: str = \"\"\n",
    "    output_dir: str = \"./explainmyxray-v2-medgemma-padchest\"\n",
    "\n",
    "    # ---- QLoRA ----\n",
    "    lora_r: int = 64       # High rank for 174 findings x 104 locations\n",
    "    lora_alpha: int = 128\n",
    "    lora_dropout: float = 0.05\n",
    "    load_in_4bit: bool = True\n",
    "    bnb_4bit_quant_type: str = \"nf4\"\n",
    "    bnb_4bit_use_double_quant: bool = True\n",
    "\n",
    "    # ---- Training (tuned for 12 GB VRAM single GPU) ----\n",
    "    num_train_epochs: int = 8\n",
    "    per_device_train_batch_size: int = 1\n",
    "    per_device_eval_batch_size: int = 1\n",
    "    gradient_accumulation_steps: int = 16   # eff batch = 1 * 1 * 16 = 16\n",
    "    learning_rate: float = 5e-5\n",
    "    warmup_ratio: float = 0.1\n",
    "    max_grad_norm: float = 0.3\n",
    "    lr_scheduler_type: str = \"cosine_with_restarts\"\n",
    "    logging_steps: int = 10\n",
    "    eval_steps: int = 50\n",
    "    save_steps: int = 100\n",
    "    max_seq_length: int = 768\n",
    "    label_smoothing_factor: float = 0.05\n",
    "\n",
    "    # ---- Image Preprocessing (same as Kaggle notebook) ----\n",
    "    image_size: int = 512\n",
    "    apply_clahe: bool = True\n",
    "    clahe_clip_limit: float = 2.0\n",
    "    clahe_grid_size: int = 8\n",
    "    auto_crop_edges: bool = True\n",
    "    edge_crop_threshold: float = 0.05\n",
    "    pad_to_square: bool = True\n",
    "    pad_color: int = 0\n",
    "\n",
    "    # ---- Splits ----\n",
    "    train_ratio: float = 0.90\n",
    "    val_ratio: float = 0.05\n",
    "    test_ratio: float = 0.05\n",
    "    max_samples: int = 0\n",
    "    use_curriculum: bool = True\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "# ---- Auto-set paths from Google Drive detection ----\n",
    "if GDRIVE_PADCHEST and cfg.use_full_padchest:\n",
    "    csv_candidates = [\n",
    "        os.path.join(GDRIVE_PADCHEST, 'PADCHEST_chest_x_ray_images_labels_160K.csv'),\n",
    "        os.path.join(GDRIVE_PADCHEST, 'padchest_labels.csv'),\n",
    "    ]\n",
    "    for c in csv_candidates:\n",
    "        if os.path.isfile(c):\n",
    "            cfg.padchest_csv = c; break\n",
    "    img_candidates = [\n",
    "        os.path.join(GDRIVE_PADCHEST, 'images'),\n",
    "        os.path.join(GDRIVE_PADCHEST, 'PadChest', 'images'),\n",
    "    ]\n",
    "    for c in img_candidates:\n",
    "        if os.path.isdir(c):\n",
    "            cfg.padchest_images = c; break\n",
    "\n",
    "# ---- Fallback: local dataset ----\n",
    "if not cfg.padchest_csv:\n",
    "    # Check common local paths (adjust to your setup)\n",
    "    local_candidates = [\n",
    "        './data/padchest/PADCHEST_chest_x_ray_images_labels_160K.csv',\n",
    "        os.path.expanduser('~/Datasets/Padchest/PADCHEST_chest_x_ray_images_labels_160K.csv'),\n",
    "        os.path.expanduser('~/data/padchest/PADCHEST_chest_x_ray_images_labels_160K.csv'),\n",
    "    ]\n",
    "    for local_csv in local_candidates:\n",
    "        if os.path.isfile(local_csv):\n",
    "            cfg.padchest_csv = local_csv\n",
    "            cfg.padchest_images = os.path.join(os.path.dirname(local_csv), 'images')\n",
    "            print(f'Using local dataset: {local_csv}')\n",
    "            break\n",
    "\n",
    "print(f'CSV: {cfg.padchest_csv} (exists: {os.path.isfile(cfg.padchest_csv) if cfg.padchest_csv else False})')\n",
    "print(f'Images: {cfg.padchest_images} (exists: {os.path.isdir(cfg.padchest_images) if cfg.padchest_images else False})')\n",
    "print(f'LoRA r={cfg.lora_r}, LR={cfg.learning_rate}, Epochs={cfg.num_train_epochs}')\n",
    "print(f'Effective batch: {cfg.per_device_train_batch_size} x {cfg.gradient_accumulation_steps} = {cfg.per_device_train_batch_size * cfg.gradient_accumulation_steps}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3 — Data-Driven Image Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 7: Medical X-Ray Preprocessing Pipeline (7 stages)\n",
    "# ============================================================\n",
    "# Same pipeline as Kaggle notebook — designed from analysis of\n",
    "# 200+ PadChest images. This ensures consistent preprocessing\n",
    "# regardless of where the model is trained.\n",
    "# ============================================================\n",
    "\n",
    "def _convert_16bit_to_8bit(img):\n",
    "    \"\"\"Convert 16-bit images to 8-bit via percentile normalization.\"\"\"\n",
    "    arr = np.array(img, dtype=np.float32)\n",
    "    if arr.ndim == 3: arr = arr[:, :, 0]\n",
    "    p1, p99 = np.percentile(arr, [1, 99])\n",
    "    if p99 - p1 < 1: p1, p99 = arr.min(), arr.max()\n",
    "    if p99 - p1 < 1: return np.zeros(arr.shape, dtype=np.uint8)\n",
    "    return np.clip((arr - p1) / (p99 - p1) * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "def _auto_crop_dark_edges(gray, threshold_ratio=0.05):\n",
    "    \"\"\"Crop dark edge artifacts (33% of PadChest images).\"\"\"\n",
    "    h, w = gray.shape\n",
    "    if h < 100 or w < 100: return gray\n",
    "    center = gray[h//4:3*h//4, w//4:3*w//4]\n",
    "    center_mean = center.mean()\n",
    "    if center_mean < 10: return gray\n",
    "    threshold = center_mean * threshold_ratio\n",
    "    top, bottom, left, right = 0, h, 0, w\n",
    "    for row in range(h // 6):\n",
    "        if gray[row, w//4:3*w//4].mean() < threshold: top = row + 1\n",
    "        else: break\n",
    "    for row in range(h - 1, h - h // 6, -1):\n",
    "        if gray[row, w//4:3*w//4].mean() < threshold: bottom = row\n",
    "        else: break\n",
    "    for col in range(w // 6):\n",
    "        if gray[h//4:3*h//4, col].mean() < threshold: left = col + 1\n",
    "        else: break\n",
    "    for col in range(w - 1, w - w // 6, -1):\n",
    "        if gray[h//4:3*h//4, col].mean() < threshold: right = col\n",
    "        else: break\n",
    "    if (bottom - top) < h * 0.6 or (right - left) < w * 0.6: return gray\n",
    "    return gray[top:bottom, left:right]\n",
    "\n",
    "def _pad_to_square(gray, pad_value=0):\n",
    "    \"\"\"Pad to square preserving aspect ratio.\"\"\"\n",
    "    h, w = gray.shape\n",
    "    if h == w: return gray\n",
    "    t = max(h, w)\n",
    "    padded = np.full((t, t), pad_value, dtype=gray.dtype)\n",
    "    padded[(t-h)//2:(t-h)//2+h, (t-w)//2:(t-w)//2+w] = gray\n",
    "    return padded\n",
    "\n",
    "def preprocess_medical_image(image_path, cfg=cfg):\n",
    "    \"\"\"Full 7-stage preprocessing pipeline.\"\"\"\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        if img.mode in ('I;16', 'I', 'I;16B', 'I;16L'):\n",
    "            gray = _convert_16bit_to_8bit(img)\n",
    "        elif img.mode == 'L': gray = np.array(img, dtype=np.uint8)\n",
    "        elif img.mode in ('RGB', 'RGBA'): gray = np.array(img.convert('L'), dtype=np.uint8)\n",
    "        else: gray = _convert_16bit_to_8bit(img)\n",
    "        if cfg.auto_crop_edges: gray = _auto_crop_dark_edges(gray, cfg.edge_crop_threshold)\n",
    "        if cfg.pad_to_square: gray = _pad_to_square(gray, cfg.pad_color)\n",
    "        img_pil = Image.fromarray(gray, mode='L').resize((cfg.image_size, cfg.image_size), Image.LANCZOS)\n",
    "        gray = np.array(img_pil, dtype=np.uint8)\n",
    "        if cfg.apply_clahe:\n",
    "            clahe = cv2.createCLAHE(clipLimit=cfg.clahe_clip_limit,\n",
    "                                   tileGridSize=(cfg.clahe_grid_size, cfg.clahe_grid_size))\n",
    "            gray = clahe.apply(gray)\n",
    "        img_pil = Image.fromarray(gray, mode='L')\n",
    "        img_pil = ImageEnhance.Sharpness(img_pil).enhance(1.2)\n",
    "        return img_pil.convert('RGB')\n",
    "    except Exception as e:\n",
    "        print(f'[WARN] Preprocess failed: {os.path.basename(str(image_path))}: {e}')\n",
    "        return None\n",
    "\n",
    "print('Pipeline: 16bit->8bit -> crop -> pad -> resize -> CLAHE -> sharpen -> RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 8: Test Preprocessing Pipeline\n",
    "# ============================================================\n",
    "test_dir = None\n",
    "if cfg.padchest_images and os.path.isdir(cfg.padchest_images):\n",
    "    for sub in sorted(os.listdir(cfg.padchest_images)):\n",
    "        sp = os.path.join(cfg.padchest_images, sub)\n",
    "        if os.path.isdir(sp) and len(os.listdir(sp)) > 0:\n",
    "            test_dir = sp; break\n",
    "    if test_dir is None and any(f.endswith('.png') for f in os.listdir(cfg.padchest_images)):\n",
    "        test_dir = cfg.padchest_images\n",
    "\n",
    "if test_dir:\n",
    "    test_files = sorted([f for f in os.listdir(test_dir) if f.lower().endswith(('.png','.jpg','.jpeg'))])[:6]\n",
    "    if test_files:\n",
    "        ncols = min(6, len(test_files))\n",
    "        fig, axes = plt.subplots(2, ncols, figsize=(4*ncols, 8))\n",
    "        if ncols == 1: axes = axes.reshape(2, 1)\n",
    "        fig.suptitle('Preprocessing: Raw vs Processed', fontsize=14, fontweight='bold')\n",
    "        for i, f in enumerate(test_files):\n",
    "            path = os.path.join(test_dir, f)\n",
    "            raw = Image.open(path)\n",
    "            raw_arr = np.array(raw, dtype=np.float32)\n",
    "            axes[0, i].imshow(raw_arr, cmap='gray')\n",
    "            axes[0, i].set_title(f'{raw.size[0]}x{raw.size[1]} {raw.mode}', fontsize=8)\n",
    "            axes[0, i].axis('off')\n",
    "            processed = preprocess_medical_image(path)\n",
    "            if processed:\n",
    "                axes[1, i].imshow(processed)\n",
    "                axes[1, i].set_title(f'{processed.size[0]}x{processed.size[1]} RGB', fontsize=8)\n",
    "            axes[1, i].axis('off')\n",
    "        plt.tight_layout(); plt.show()\n",
    "else:\n",
    "    print('No images found to test pipeline.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 9: Load PadChest CSV & Parse Labels\n",
    "# ============================================================\n",
    "df = pd.read_csv(cfg.padchest_csv)\n",
    "print(f\"Raw: {len(df)} rows, {len(df.columns)} columns\")\n",
    "\n",
    "def safe_parse_list(val):\n",
    "    if pd.isna(val) or str(val).strip() in [\"\",\"[]\",\"nan\",\"None\"]: return []\n",
    "    try:\n",
    "        parsed = ast.literal_eval(str(val))\n",
    "        if isinstance(parsed, list):\n",
    "            flat = []\n",
    "            for item in parsed:\n",
    "                if isinstance(item, list): flat.extend(str(x).strip() for x in item)\n",
    "                else: flat.append(str(item).strip())\n",
    "            return [f for f in flat if f and f != 'nan']\n",
    "        return [str(parsed).strip()]\n",
    "    except: return [str(val).strip()]\n",
    "\n",
    "df[\"labels_parsed\"] = df[\"Labels\"].apply(safe_parse_list)\n",
    "df[\"localizations_parsed\"] = df[\"Localizations\"].apply(safe_parse_list)\n",
    "df[\"labels_locs_parsed\"] = df[\"LabelsLocalizationsBySentence\"].apply(safe_parse_list)\n",
    "\n",
    "def split_findings_locations(items):\n",
    "    findings, locations = [], []\n",
    "    for item in items:\n",
    "        c = item.strip()\n",
    "        if c.startswith(\"loc \"): locations.append(c[4:].strip())\n",
    "        elif c not in [\"exclude\",\"\",\"nan\"]: findings.append(c)\n",
    "    return findings, locations\n",
    "\n",
    "df[\"findings\"], df[\"locations\"] = zip(*df[\"labels_locs_parsed\"].apply(split_findings_locations))\n",
    "df[\"num_findings\"] = df[\"findings\"].apply(len)\n",
    "print(f\"Parsed: {len(df)} rows, findings range {df[\"num_findings\"].min()}-{df[\"num_findings\"].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 10: Filter Valid Images\n",
    "# ============================================================\n",
    "def resolve_image_path(row):\n",
    "    img_name = row[\"ImageID\"]\n",
    "    if cfg.use_full_padchest:\n",
    "        img_dir_num = row.get(\"ImageDir\")\n",
    "        if pd.notna(img_dir_num):\n",
    "            return os.path.join(cfg.padchest_images, str(int(img_dir_num)), img_name)\n",
    "        for sub in range(38):\n",
    "            c = os.path.join(cfg.padchest_images, str(sub), img_name)\n",
    "            if os.path.exists(c): return c\n",
    "    return os.path.join(cfg.padchest_images, img_name)\n",
    "\n",
    "df[\"image_path\"] = df.apply(resolve_image_path, axis=1)\n",
    "df[\"image_exists\"] = df[\"image_path\"].apply(os.path.exists)\n",
    "df_valid = df[df[\"image_exists\"]].copy()\n",
    "print(f\"Available: {len(df_valid)} / {len(df)}\")\n",
    "\n",
    "if len(df_valid) == 0:\n",
    "    raise FileNotFoundError(\"No images found! Check paths in Config.\")\n",
    "\n",
    "df_valid = df_valid[df_valid[\"num_findings\"] > 0].copy()\n",
    "print(f\"With findings: {len(df_valid)}\")\n",
    "if cfg.max_samples > 0:\n",
    "    df_valid = df_valid.sample(n=min(cfg.max_samples, len(df_valid)), random_state=SEED)\n",
    "\n",
    "all_findings = [f for fl in df_valid[\"findings\"] for f in fl]\n",
    "finding_counts = Counter(all_findings)\n",
    "print(f\"Unique findings: {len(finding_counts)}\")\n",
    "for f, c in finding_counts.most_common(15):\n",
    "    print(f\"  {f}: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 11: Structured Prompt Engineering\n",
    "# ============================================================\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are an expert board-certified radiologist AI analyzing chest X-rays \"\n",
    "    \"from the BIMCV PadChest dataset. Produce a structured radiology report \"\n",
    "    \"following this exact format:\\n\\n\"\n",
    "    \"FINDINGS:\\n\"\n",
    "    \"- State each finding on a separate line\\n\"\n",
    "    \"- Include anatomical location in parentheses when known\\n\"\n",
    "    \"- Be specific: use standard radiological terminology\\n\"\n",
    "    \"- If no abnormality: state 'No significant abnormalities detected'\\n\\n\"\n",
    "    \"LOCATIONS:\\n\"\n",
    "    \"- List all affected anatomical regions\\n\\n\"\n",
    "    \"IMPRESSION:\\n\"\n",
    "    \"- Provide a concise clinical summary\\n\"\n",
    "    \"- Note if correlation with prior studies is recommended\\n\\n\"\n",
    "    \"Be systematic: check lung fields, mediastinum, cardiac silhouette, \"\n",
    "    \"diaphragm, pleural spaces, and bony thorax.\"\n",
    ")\n",
    "\n",
    "def build_user_prompt(view_position='PA'):\n",
    "    v = view_position if pd.notna(view_position) and view_position else \"unknown\"\n",
    "    return f\"Analyze this chest X-ray (projection: {v}). Provide FINDINGS, LOCATIONS, and IMPRESSION.\"\n",
    "\n",
    "def build_assistant_response(findings, locations):\n",
    "    fu = list(dict.fromkeys(findings))\n",
    "    lu = list(dict.fromkeys(locations))\n",
    "    abn = [f for f in fu if f.lower() not in [\"normal\",\"unchanged\",\"exclude\",\"nan\",\"\"]]\n",
    "    nl = chr(10)\n",
    "    if not abn:\n",
    "        fs = '- No significant abnormalities detected'\n",
    "        imp = 'Normal chest X-ray. No acute cardiopulmonary disease.'\n",
    "    else:\n",
    "        lines = []\n",
    "        for f in abn:\n",
    "            matched = [l for l in lu if l]\n",
    "            loc_joined = ', '.join(matched[:3])\n",
    "            loc_str = f' ({loc_joined})' if matched else ''\n",
    "            lines.append(f'- {f.capitalize()}{loc_str}')\n",
    "        fs = nl.join(lines)\n",
    "        if len(abn) == 1:\n",
    "            imp = f'{abn[0].capitalize()} identified. Clinical correlation recommended.'\n",
    "        else:\n",
    "            top = ', '.join(a.capitalize() for a in abn[:4])\n",
    "            imp = f'Multiple findings: {top}. Clinical correlation and follow-up recommended.'\n",
    "    r = f'FINDINGS:{nl}{fs}{nl}{nl}'\n",
    "    locs_str = ', '.join(lu) if lu else 'Not specified'\n",
    "    r += f'LOCATIONS:{nl}{locs_str}{nl}{nl}'\n",
    "    r += f'IMPRESSION:{nl}{imp}'\n",
    "    return r\n",
    "\n",
    "s = df_valid.iloc[0]\n",
    "print(\"=== Sample ===\")\n",
    "print(f\"Findings: {s[\"findings\"]}\")\n",
    "print(f\"\\nOutput:\\n{build_assistant_response(s[\"findings\"], s[\"locations\"])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 12: Curriculum Learning\n",
    "# ============================================================\n",
    "def compute_difficulty(row):\n",
    "    score = 0\n",
    "    findings = row[\"findings\"]\n",
    "    normal_labels = {\"normal\",\"unchanged\",\"exclude\",\"nan\",\"\"}\n",
    "    abn = [f for f in findings if f.lower() not in normal_labels]\n",
    "    score += len(abn) * 2\n",
    "    score += len(row[\"locations\"])\n",
    "    for f in abn:\n",
    "        freq = finding_counts.get(f, 0)\n",
    "        if freq <= 5: score += 5\n",
    "        elif freq <= 20: score += 3\n",
    "        elif freq <= 50: score += 1\n",
    "    return score\n",
    "\n",
    "df_valid[\"difficulty\"] = df_valid.apply(compute_difficulty, axis=1)\n",
    "if cfg.use_curriculum:\n",
    "    df_valid = df_valid.sort_values(\"difficulty\").reset_index(drop=True)\n",
    "    print('Curriculum: easy -> hard')\n",
    "else:\n",
    "    df_valid = df_valid.sample(frac=1, random_state=SEED).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 13: Convert to HuggingFace Dataset (with preprocessing)\n",
    "# ============================================================\n",
    "def row_to_example(row):\n",
    "    try:\n",
    "        image = preprocess_medical_image(row[\"image_path\"], cfg)\n",
    "        if image is None: return None\n",
    "    except Exception as e:\n",
    "        print(f\"  Skip: {e}\")\n",
    "        return None\n",
    "    v = row.get(\"Projection\", \"PA\")\n",
    "    messages = [\n",
    "        {\"role\":\"system\",\"content\":[{\"type\":\"text\",\"text\":SYSTEM_PROMPT}]},\n",
    "        {\"role\":\"user\",\"content\":[{\"type\":\"image\"},{\"type\":\"text\",\"text\":build_user_prompt(v)}]},\n",
    "        {\"role\":\"assistant\",\"content\":[{\"type\":\"text\",\"text\":build_assistant_response(row[\"findings\"],row[\"locations\"])}]},\n",
    "    ]\n",
    "    return {\"image\":image,\"messages\":messages,\"findings\":row[\"findings\"],\"locations\":row[\"locations\"],\"image_id\":row[\"ImageID\"]}\n",
    "\n",
    "print(\"Processing images...\")\n",
    "t0 = time.time()\n",
    "examples, failed = [], 0\n",
    "for i, (_, row) in enumerate(df_valid.iterrows()):\n",
    "    ex = row_to_example(row)\n",
    "    if ex: examples.append(ex)\n",
    "    else: failed += 1\n",
    "    if (i+1) % 500 == 0:\n",
    "        print(f'  {i+1}/{len(df_valid)} | ok={len(examples)} fail={failed}')\n",
    "\n",
    "print(f'Done: {len(examples)}/{len(df_valid)} in {time.time()-t0:.0f}s ({failed} skipped)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 14: Train / Val / Test Split\n",
    "# ============================================================\n",
    "n = len(examples)\n",
    "n_train = int(n * cfg.train_ratio)\n",
    "n_val = int(n * cfg.val_ratio)\n",
    "train_ex = examples[:n_train]\n",
    "val_ex = examples[n_train:n_train+n_val]\n",
    "test_ex = examples[n_train+n_val:]\n",
    "random.shuffle(val_ex); random.shuffle(test_ex)\n",
    "\n",
    "def to_ds(exs):\n",
    "    return Dataset.from_dict({\n",
    "        \"image\":[e[\"image\"] for e in exs],\n",
    "        \"messages\":[e[\"messages\"] for e in exs],\n",
    "        \"findings\":[e[\"findings\"] for e in exs],\n",
    "        \"locations\":[e[\"locations\"] for e in exs],\n",
    "        \"image_id\":[e[\"image_id\"] for e in exs],\n",
    "    })\n",
    "\n",
    "dataset = DatasetDict({\"train\":to_ds(train_ex),\"validation\":to_ds(val_ex),\"test\":to_ds(test_ex)})\n",
    "for split, ds in dataset.items(): print(f'  {split}: {len(ds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4 — Model Loading (MedGemma-4B QLoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 15: Load MedGemma-4B-it with QLoRA\n",
    "# ============================================================\n",
    "# RTX 4080 = cc 8.9 → use bfloat16\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=COMPUTE_DTYPE,\n",
    "    bnb_4bit_quant_storage=COMPUTE_DTYPE,\n",
    ")\n",
    "\n",
    "print(f'Loading {cfg.model_id} with 4-bit NF4 ({COMPUTE_DTYPE})...')\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    cfg.model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    attn_implementation=\"eager\",  # Required for MedGemma\n",
    "    torch_dtype=COMPUTE_DTYPE,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(cfg.model_id)\n",
    "processor.tokenizer.padding_side = \"right\"\n",
    "print(f'Model loaded: {torch.cuda.memory_allocated()/1e9:.2f} GB VRAM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 16: LoRA Config — r=64\n",
    "# ============================================================\n",
    "peft_config = LoraConfig(\n",
    "    r=cfg.lora_r,\n",
    "    lora_alpha=cfg.lora_alpha,\n",
    "    lora_dropout=cfg.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\"lm_head\",\"embed_tokens\"],\n",
    ")\n",
    "print(f'LoRA: r={cfg.lora_r}, alpha={cfg.lora_alpha}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 17: Data Collator\n",
    "# ============================================================\n",
    "def collate_fn(examples):\n",
    "    texts, images = [], []\n",
    "    for ex in examples:\n",
    "        img = ex[\"image\"]\n",
    "        if not isinstance(img, Image.Image):\n",
    "            img = Image.open(img).convert(\"RGB\")\n",
    "        images.append([img.convert(\"RGB\")])\n",
    "        text = processor.apply_chat_template(\n",
    "            ex[\"messages\"], add_generation_prompt=False, tokenize=False\n",
    "        ).strip()\n",
    "        texts.append(text)\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\",\n",
    "                      padding=True, truncation=True, max_length=cfg.max_seq_length)\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    pid = processor.tokenizer.pad_token_id\n",
    "    if pid is not None: labels[labels == pid] = -100\n",
    "    labels[labels == 262144] = -100\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n",
    "\n",
    "tb = collate_fn([dataset[\"train\"][0]])\n",
    "print(f\"Collator OK. Input: {tb[\"input_ids\"].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5 — Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 18: Training Arguments (RTX 4080, 12 GB VRAM)\n",
    "# ============================================================\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=cfg.output_dir,\n",
    "    num_train_epochs=cfg.num_train_epochs,\n",
    "    per_device_train_batch_size=cfg.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=cfg.per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=cfg.gradient_accumulation_steps,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=cfg.learning_rate,\n",
    "    warmup_ratio=cfg.warmup_ratio,\n",
    "    max_grad_norm=cfg.max_grad_norm,\n",
    "    lr_scheduler_type=cfg.lr_scheduler_type,\n",
    "    weight_decay=0.01,\n",
    "    label_smoothing_factor=cfg.label_smoothing_factor,\n",
    "    bf16=USE_BF16,\n",
    "    fp16=not USE_BF16,\n",
    "    logging_steps=cfg.logging_steps,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=cfg.eval_steps,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=cfg.save_steps,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    logging_dir=os.path.join(cfg.output_dir, \"logs\"),\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=4,  # Desktop has more CPU cores\n",
    "    max_seq_length=cfg.max_seq_length,\n",
    ")\n",
    "\n",
    "early_stop = EarlyStoppingCallback(early_stopping_patience=5, early_stopping_threshold=0.001)\n",
    "eff = cfg.per_device_train_batch_size * cfg.gradient_accumulation_steps\n",
    "print(f'Epochs: {cfg.num_train_epochs}, Effective batch: {eff}, LR: {cfg.learning_rate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 19: Build Trainer & Start Training\n",
    "# ============================================================\n",
    "trainer = SFTTrainer(\n",
    "    model=model, args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    peft_config=peft_config,\n",
    "    processing_class=processor,\n",
    "    data_collator=collate_fn,\n",
    "    callbacks=[early_stop],\n",
    ")\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_p = sum(p.numel() for p in model.parameters())\n",
    "print(f'Trainable: {trainable:,} / {total_p:,} ({100*trainable/total_p:.2f}%)')\n",
    "print(f'VRAM: {torch.cuda.memory_allocated()/1e9:.2f} GB')\n",
    "\n",
    "print('Starting training...')\n",
    "train_result = trainer.train()\n",
    "print(f'Done! Loss: {train_result.training_loss:.4f}, Steps: {train_result.global_step}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 20: Save Model\n",
    "# ============================================================\n",
    "trainer.save_model(cfg.output_dir)\n",
    "processor.save_pretrained(cfg.output_dir)\n",
    "size_mb = sum(os.path.getsize(os.path.join(cfg.output_dir, f))\n",
    "    for f in os.listdir(cfg.output_dir)\n",
    "    if os.path.isfile(os.path.join(cfg.output_dir, f))) / 1e6\n",
    "print(f'Saved: {cfg.output_dir} ({size_mb:.1f} MB)')\n",
    "del model, trainer\n",
    "torch.cuda.empty_cache(); gc.collect()\n",
    "print(f'VRAM freed: {torch.cuda.memory_allocated()/1e9:.2f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 6 — Evaluation & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 21: Load Fine-Tuned Model for Inference\n",
    "# ============================================================\n",
    "ft_pipe = pipeline('image-text-to-text', model=cfg.output_dir,\n",
    "                   torch_dtype=COMPUTE_DTYPE, device_map='auto')\n",
    "ft_pipe.model.generation_config.do_sample = False\n",
    "ft_pipe.model.generation_config.max_new_tokens = 384\n",
    "ft_pipe.model.generation_config.temperature = 0.1\n",
    "ft_pipe.tokenizer.padding_side = 'left'\n",
    "print('Fine-tuned model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 22: Run Inference on Test Set\n",
    "# ============================================================\n",
    "def run_inference(pipe, ds, max_n=None):\n",
    "    results = []\n",
    "    n = min(len(ds), max_n) if max_n else len(ds)\n",
    "    for i in range(n):\n",
    "        ex = ds[i]\n",
    "        msgs = [\n",
    "            {\"role\":\"system\",\"content\":[{\"type\":\"text\",\"text\":SYSTEM_PROMPT}]},\n",
    "            {\"role\":\"user\",\"content\":[{\"type\":\"image\"},\n",
    "             {\"type\":\"text\",\"text\":build_user_prompt(\"PA\")}]},\n",
    "        ]\n",
    "        try:\n",
    "            out = pipe(text=msgs, images=[ex['image']], return_full_text=False)\n",
    "            pred = out[0]['generated_text']\n",
    "        except Exception as e: pred = f'ERROR: {e}'\n",
    "        results.append({\n",
    "            'image_id': ex['image_id'], 'gt_findings': ex['findings'],\n",
    "            'gt_locations': ex['locations'], 'prediction': pred,\n",
    "        })\n",
    "        if (i+1) % 5 == 0: print(f'  {i+1}/{n}')\n",
    "    return results\n",
    "\n",
    "print(f'Inference on {len(dataset[\"test\"])} test samples...')\n",
    "test_results = run_inference(ft_pipe, dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 23: Comprehensive Evaluation\n",
    "# ============================================================\n",
    "def extract_findings_from_report(text):\n",
    "    findings, in_findings = [], False\n",
    "    for line in text.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if line.upper().startswith('FINDINGS'): in_findings = True; continue\n",
    "        if line.upper().startswith(('LOCATIONS','IMPRESSION')): in_findings = False; continue\n",
    "        if in_findings and line.startswith('- '):\n",
    "            f = line.lstrip('- ').split('(')[0].strip().lower()\n",
    "            if f and f not in ['nan','']: findings.append(f)\n",
    "        elif in_findings and 'no significant' in line.lower():\n",
    "            findings.append('normal')\n",
    "    return findings if findings else ['normal']\n",
    "\n",
    "exact_match, soft_match, total = 0, 0, 0\n",
    "tp, fp, fn = Counter(), Counter(), Counter()\n",
    "per_finding_tp, per_finding_total = Counter(), Counter()\n",
    "\n",
    "for r in test_results:\n",
    "    gt = set(f.lower().strip() for f in r['gt_findings'] if f.strip())\n",
    "    pr = set(extract_findings_from_report(r['prediction']))\n",
    "    if gt == pr: exact_match += 1\n",
    "    if gt:\n",
    "        overlap = len(gt & pr) / len(gt)\n",
    "        if overlap >= 0.5: soft_match += 1\n",
    "    else:\n",
    "        if not pr or pr == {'normal'}: soft_match += 1\n",
    "    total += 1\n",
    "    for f in gt & pr: tp[f] += 1\n",
    "    for f in pr - gt: fp[f] += 1\n",
    "    for f in gt - pr: fn[f] += 1\n",
    "    for f in gt: per_finding_total[f] += 1\n",
    "    for f in gt & pr: per_finding_tp[f] += 1\n",
    "\n",
    "ea = exact_match / total if total else 0\n",
    "sa = soft_match / total if total else 0\n",
    "ttp, tfp, tfn = sum(tp.values()), sum(fp.values()), sum(fn.values())\n",
    "prec = ttp / (ttp + tfp) if ttp + tfp else 0\n",
    "rec = ttp / (ttp + tfn) if ttp + tfn else 0\n",
    "f1 = 2 * prec * rec / (prec + rec) if prec + rec else 0\n",
    "\n",
    "print('='*60)\n",
    "print(f'  RESULTS ({total} test samples)')\n",
    "print('='*60)\n",
    "print(f'  Exact match:  {exact_match}/{total} ({ea*100:.1f}%)')\n",
    "print(f'  Soft match:   {soft_match}/{total} ({sa*100:.1f}%)')\n",
    "print(f'  Precision:    {prec:.3f}')\n",
    "print(f'  Recall:       {rec:.3f}')\n",
    "print(f'  F1:           {f1:.3f}')\n",
    "print('='*60)\n",
    "if sa >= 0.95: print(f'  TARGET MET: {sa*100:.1f}% >= 95%')\n",
    "elif sa >= 0.90: print(f'  CLOSE: {sa*100:.1f}% (gap: {(0.95-sa)*100:.1f}%)')\n",
    "else: print(f'  Gap: {(0.95-sa)*100:.1f}% to 95%')\n",
    "\n",
    "print(f'\\nPer-finding accuracy (top 20):')\n",
    "for f in sorted(per_finding_total, key=lambda x: per_finding_total[x], reverse=True)[:20]:\n",
    "    t = per_finding_tp.get(f, 0)\n",
    "    n = per_finding_total[f]\n",
    "    acc = t/n if n else 0\n",
    "    bar = chr(9608) * int(acc * 20)\n",
    "    print(f'  {f:30s}: {t:3d}/{n:3d} ({acc*100:5.1f}%) {bar}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 24: Visualize Predictions\n",
    "# ============================================================\n",
    "ANAT_REGIONS = {\n",
    "    'right upper lobe': (0.05,0.05,0.35,0.30),\n",
    "    'right middle lobe': (0.05,0.30,0.35,0.20),\n",
    "    'right lower lobe': (0.05,0.50,0.35,0.30),\n",
    "    'left upper lobe':  (0.55,0.05,0.35,0.30),\n",
    "    'left lower lobe':  (0.55,0.50,0.35,0.30),\n",
    "    'cardiac silhouette': (0.30,0.30,0.35,0.40),\n",
    "    'cardiac': (0.30,0.30,0.35,0.40),\n",
    "    'bilateral': (0.05,0.15,0.85,0.65),\n",
    "    'hilar': (0.30,0.20,0.35,0.30),\n",
    "    'mediastinum': (0.30,0.10,0.35,0.60),\n",
    "}\n",
    "COLORS = ['#FF4444','#4488FF','#FF8800','#FFCC00','#44FF88','#FF44FF','#88FFFF']\n",
    "\n",
    "for i in range(min(5, len(test_results))):\n",
    "    r = test_results[i]\n",
    "    img = dataset['test'][i]['image']\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 7))\n",
    "    axes[0].imshow(img); axes[0].set_title('Input'); axes[0].axis('off')\n",
    "    axes[1].imshow(img)\n",
    "    w, h = (img.size if hasattr(img,'size') else (img.shape[1],img.shape[0]))\n",
    "    for j, loc in enumerate(r['gt_locations']):\n",
    "        k = loc.lower().strip()\n",
    "        if k in ANAT_REGIONS:\n",
    "            rx,ry,rw,rh = ANAT_REGIONS[k]\n",
    "            c = COLORS[j % len(COLORS)]\n",
    "            axes[1].add_patch(Rectangle((rx*w,ry*h),rw*w,rh*h,lw=2,ec=c,fc=c,alpha=0.15))\n",
    "    findings_str = ', '.join(r['gt_findings'][:3])\n",
    "    axes[1].set_title(f'Ground Truth: {findings_str}'); axes[1].axis('off')\n",
    "    axes[2].text(0.05,0.95,r['prediction'],transform=axes[2].transAxes,fontsize=8,\n",
    "                va='top',fontfamily='monospace',wrap=True)\n",
    "    axes[2].set_title('Prediction'); axes[2].axis('off')\n",
    "    fig.suptitle(f'Test #{i+1}: {r[\"image_id\"]}', fontsize=13, fontweight='bold')\n",
    "    plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 25: Interactive Prediction\n",
    "# ============================================================\n",
    "def predict_xray(image_path, view='PA'):\n",
    "    \"\"\"Analyze any chest X-ray with the fine-tuned model.\"\"\"\n",
    "    img = preprocess_medical_image(image_path, cfg)\n",
    "    if img is None: img = Image.open(image_path).convert('RGB')\n",
    "    msgs = [\n",
    "        {'role':'system','content':[{'type':'text','text':SYSTEM_PROMPT}]},\n",
    "        {'role':'user','content':[{'type':'image'},\n",
    "         {'type':'text','text':build_user_prompt(view)}]},\n",
    "    ]\n",
    "    out = ft_pipe(text=msgs, images=[img], return_full_text=False)\n",
    "    report = out[0]['generated_text']\n",
    "    print(report)\n",
    "    return report, img\n",
    "\n",
    "print('predict_xray() ready.')\n",
    "print('Usage: report, img = predict_xray(\"path/to/xray.png\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Component | RTX 4080 | Kaggle T4x2 |\n",
    "|-----------|----------|-------------|\n",
    "| GPU | RTX 4080 12GB (bf16) | 2x T4 15GB (fp16) |\n",
    "| LoRA | r=64, alpha=128 | r=64, alpha=128 |\n",
    "| Batch | 1 x 16 accum = 16 | 1 x 2 GPU x 8 accum = 16 |\n",
    "| Preprocessing | 7-stage identical | 7-stage identical |\n",
    "| Data | GDrive Desktop / local | GDrive via gdown / Kaggle Dataset |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}