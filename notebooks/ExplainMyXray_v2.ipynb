{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38ec2c18",
   "metadata": {},
   "source": [
    "# ExplainMyXray v2 — MedGemma-4B + PadChest + Disease Localization\n",
    "\n",
    "**Complete rebuild** of the ExplainMyXray training pipeline.  \n",
    "Target hardware: **RTX 4080 Laptop 12 GB VRAM** (local Windows/Linux).  \n",
    "Model: **google/medgemma-4b-it** (Gemma 3 decoder + medical SigLIP encoder).  \n",
    "Dataset: **BIMCV PadChest** — 160K+ CXR images, 174 radiographic findings, 104 anatomical locations.  \n",
    "Training: **QLoRA 4-bit NF4** via TRL `SFTTrainer`, bfloat16, gradient checkpointing.\n",
    "\n",
    "---\n",
    "\n",
    "## V1 Brutal Evaluation — Why We Rebuilt Everything\n",
    "\n",
    "| Flaw | Impact | v2 Fix |\n",
    "|------|--------|--------|\n",
    "| **Wrong model** — PaliGemma-3B is a general VLM with zero medical pre-training | Learns chest X-rays from scratch; wastes capacity | MedGemma-4B pre-trained on CXR, derm, ophtho, histo |\n",
    "| **Tiny dataset** — ~11K images (NIH sample + Kaggle Pneumonia) | Massive overfitting (loss 2.9→0.05 in 253 steps = memorization) | PadChest 160K+ images, 174 findings |\n",
    "| **Template reports** — Hardcoded `f'The X-ray shows {label}'` strings | Model learns templates, not radiology language | Structured multi-field outputs from real annotations |\n",
    "| **No localization** — Zero bounding box or anatomical region support | Cannot point to *where* disease is | PadChest LabelsLocalizationsBySentence + anatomical overlay |\n",
    "| **Wrong API** — `PaliGemmaForConditionalGeneration` is deprecated for MedGemma | Broken upgrade path | `AutoModelForImageTextToText` + chat template |\n",
    "| **FP16 on T4** — Works but bfloat16 is superior for training stability | Occasional loss spikes, gradient overflow | BFloat16 native on RTX 4080 Laptop (SM ≥8.0) |\n",
    "| **Basic LoRA** — r=16, only attention projections | Under-adapts the vision encoder | r=16 `all-linear` + `lm_head`/`embed_tokens` saved |\n",
    "| **No evaluation** — `do_eval=False`, no metrics during training | Blind training, no early stopping signal | Eval every 50 steps with accuracy + F1 |\n",
    "| **Prediction failures** — 'No Finding' predicted as 'Pneumothorax', 'Normal' as 'Pneumonia' | Confused on similar-looking X-rays | Curriculum learning: easy→hard, multi-label classification |\n",
    "| **Processor warnings** — PaliGemmaProcessor `<image>` token spam | Flooding logs, indicates misuse | MedGemma `AutoProcessor` + `apply_chat_template` |\n",
    "| **Imbalanced eval** — Infiltration=3 vs No Finding=17 in test set | Evaluation metrics meaningless | Stratified splits, class-weighted analysis |\n",
    "\n",
    "**Bottom line**: v1 is a demo that memorized template strings on 11K images.  \n",
    "v2 is a medical AI system trained on real radiologist annotations at scale.\n",
    "\n",
    "---\n",
    "\n",
    "## v2 Architecture Overview\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────┐\n",
    "│  MedGemma-4B-it (QLoRA 4-bit NF4)              │\n",
    "│  ┌───────────────┐  ┌────────────────────────┐  │\n",
    "│  │ Medical SigLIP │→│ Gemma 3 Decoder (4B)   │  │\n",
    "│  │ Vision Encoder │  │ LoRA all-linear r=16   │  │\n",
    "│  │ 896×896 input  │  │ + lm_head/embed_tokens │  │\n",
    "│  └───────────────┘  └────────────────────────┘  │\n",
    "├─────────────────────────────────────────────────┤\n",
    "│  Multi-Task Output:                             │\n",
    "│  • Findings: [list of diagnoses]                │\n",
    "│  • Locations: [anatomical regions per finding]  │\n",
    "│  • Severity: [normal/mild/moderate/severe]      │\n",
    "│  • Report: [structured radiology summary]       │\n",
    "└─────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173cd63e",
   "metadata": {},
   "source": [
    "## Phase 1 — Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af33dc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 1: Install Dependencies + Detect Dataset Source\n",
    "# MedGemma requires transformers >= 4.50\n",
    "# TRL SFTTrainer is the official fine-tuning method\n",
    "# ============================================================\n",
    "import subprocess, sys, os\n",
    "\n",
    "packages = [\n",
    "    \"transformers>=4.52.0\",\n",
    "    \"trl>=0.17.0\",\n",
    "    \"peft>=0.15.0\",\n",
    "    \"accelerate>=1.5.0\",\n",
    "    \"bitsandbytes>=0.45.0\",\n",
    "    \"datasets>=3.5.0\",\n",
    "    \"evaluate\",\n",
    "    \"tensorboard\",\n",
    "    \"scikit-learn\",\n",
    "    \"Pillow>=10.0\",\n",
    "    \"matplotlib\",\n",
    "    \"pandas\",\n",
    "    \"gdown\",  # Google Drive downloads\n",
    "]\n",
    "\n",
    "for pkg in packages:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "\n",
    "print(\"✅ All dependencies installed.\")\n",
    "\n",
    "# ---- Mount Google Drive (Colab) or Detect Google Drive for Desktop ----\n",
    "import platform, pathlib, string\n",
    "\n",
    "IS_COLAB = False\n",
    "GDRIVE_PADCHEST = None  # Will be set if auto-detected\n",
    "\n",
    "# Priority 1: Google Colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IS_COLAB = True\n",
    "    if not os.path.exists(\"/content/drive/MyDrive\"):\n",
    "        drive.mount(\"/content/drive\")\n",
    "        print(\"✅ Google Drive mounted at /content/drive\")\n",
    "    else:\n",
    "        print(\"✅ Google Drive already mounted.\")\n",
    "    if os.path.isdir(\"/content/drive/MyDrive/Padchest\"):\n",
    "        GDRIVE_PADCHEST = \"/content/drive/MyDrive/Padchest\"\n",
    "        print(f\"✅ PadChest dataset found: {GDRIVE_PADCHEST}\")\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# Priority 2: Google Drive for Desktop (local VS Code)\n",
    "if GDRIVE_PADCHEST is None:\n",
    "    def _scan_gdrive_desktop():\n",
    "        \"\"\"Auto-detect PadChest dataset on Google Drive for Desktop.\"\"\"\n",
    "        system = platform.system()\n",
    "\n",
    "        if system == \"Windows\":\n",
    "            # Google Drive for Desktop creates a virtual drive letter (G:, H:, etc.)\n",
    "            # Scan all drive letters for \"My Drive/Padchest\"\n",
    "            for letter in string.ascii_uppercase:\n",
    "                candidate = f\"{letter}:/My Drive/Padchest\"\n",
    "                if os.path.isdir(candidate):\n",
    "                    return candidate\n",
    "            # Also check Shared drives / alternative names\n",
    "            for letter in string.ascii_uppercase:\n",
    "                for name in [\"Google Drive/My Drive/Padchest\", \"GoogleDrive/My Drive/Padchest\"]:\n",
    "                    candidate = f\"{letter}:/{name}\"\n",
    "                    if os.path.isdir(candidate):\n",
    "                        return candidate\n",
    "\n",
    "        elif system == \"Darwin\":  # macOS\n",
    "            # Modern macOS: ~/Library/CloudStorage/GoogleDrive-<email>/My Drive/\n",
    "            cloud_storage = pathlib.Path.home() / \"Library\" / \"CloudStorage\"\n",
    "            if cloud_storage.exists():\n",
    "                for folder in sorted(cloud_storage.iterdir()):\n",
    "                    if folder.name.startswith(\"GoogleDrive\"):\n",
    "                        candidate = folder / \"My Drive\" / \"Padchest\"\n",
    "                        if candidate.is_dir():\n",
    "                            return str(candidate)\n",
    "            # Legacy macOS path\n",
    "            legacy = \"/Volumes/GoogleDrive/My Drive/Padchest\"\n",
    "            if os.path.isdir(legacy):\n",
    "                return legacy\n",
    "\n",
    "        elif system == \"Linux\":\n",
    "            # Google Drive for Desktop on Linux (less common)\n",
    "            home = pathlib.Path.home()\n",
    "            linux_candidates = [\n",
    "                home / \"Google Drive\" / \"My Drive\" / \"Padchest\",\n",
    "                home / \"google-drive\" / \"My Drive\" / \"Padchest\",\n",
    "                home / \"gdrive\" / \"My Drive\" / \"Padchest\",\n",
    "                pathlib.Path(\"/mnt\") / \"google-drive\" / \"My Drive\" / \"Padchest\",\n",
    "                pathlib.Path(\"/mnt\") / \"gdrive\" / \"My Drive\" / \"Padchest\",\n",
    "            ]\n",
    "            for candidate in linux_candidates:\n",
    "                if candidate.is_dir():\n",
    "                    return str(candidate)\n",
    "\n",
    "        return None\n",
    "\n",
    "    GDRIVE_PADCHEST = _scan_gdrive_desktop()\n",
    "    if GDRIVE_PADCHEST is not None:\n",
    "        print(f\"✅ Google Drive for Desktop detected!\")\n",
    "        print(f\"   PadChest dataset: {GDRIVE_PADCHEST}\")\n",
    "        print(f\"   (Files stream on-demand — no local download needed)\")\n",
    "    else:\n",
    "        print(\"ℹ️  Google Drive for Desktop not detected.\")\n",
    "        print(\"   Options:\")\n",
    "        print(\"   1. Install Google Drive for Desktop → https://www.google.com/drive/download/\")\n",
    "        print(\"      Sign in → PadChest dataset streams automatically (recommended)\")\n",
    "        print(\"   2. Set paths manually in Cell 5 Config if Drive uses a non-standard location\")\n",
    "        print(\"   3. Download dataset locally as a last resort (needs ~300 GB)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dd3dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 2: Imports\n",
    "# ============================================================\n",
    "import os\n",
    "import ast\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import Rectangle\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    AutoModelForImageTextToText,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"transformers\")\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0edad33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 3: HuggingFace Authentication\n",
    "# MedGemma requires accepting the license at:\n",
    "#   https://huggingface.co/google/medgemma-4b-it\n",
    "# Set HF_TOKEN as environment variable or login interactively.\n",
    "# NEVER hardcode tokens in notebooks.\n",
    "# ============================================================\n",
    "from huggingface_hub import get_token, notebook_login\n",
    "\n",
    "if os.environ.get(\"HF_TOKEN\"):\n",
    "    print(\"Using HF_TOKEN from environment.\")\n",
    "elif get_token() is not None:\n",
    "    print(\"Using cached HuggingFace token.\")\n",
    "else:\n",
    "    print(\"No token found — please login:\")\n",
    "    notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b88a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 4: GPU Configuration & Memory Optimization\n",
    "# RTX 4080 Laptop: 12 GB VRAM, Ada Lovelace arch, compute cap >= 8.0\n",
    "# ============================================================\n",
    "\n",
    "# Verify GPU supports bfloat16 (compute capability >= 8.0)\n",
    "if torch.cuda.is_available():\n",
    "    cc = torch.cuda.get_device_capability()\n",
    "    if cc[0] < 8:\n",
    "        raise RuntimeError(\n",
    "            f\"GPU compute capability {cc[0]}.{cc[1]} < 8.0. \"\n",
    "            f\"MedGemma QLoRA requires bfloat16 support (RTX 30xx+).\"\n",
    "        )\n",
    "    print(f\"GPU OK: compute capability {cc[0]}.{cc[1]}\")\n",
    "else:\n",
    "    raise RuntimeError(\"No CUDA GPU detected. MedGemma requires a GPU.\")\n",
    "\n",
    "# Memory optimization for 12 GB VRAM\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"Free VRAM: {torch.cuda.mem_get_info()[0] / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d745c4",
   "metadata": {},
   "source": [
    "## Phase 2 — Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174c156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 5: Master Configuration\n",
    "# All hyperparameters and paths in one place.\n",
    "# Paths are auto-detected from Cell 1 (GDRIVE_PADCHEST).\n",
    "# ============================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Central configuration for ExplainMyXray v2.\"\"\"\n",
    "\n",
    "    # ---- Model ----\n",
    "    model_id: str = \"google/medgemma-4b-it\"\n",
    "\n",
    "    # ---- Dataset Paths ----\n",
    "    # Option A: Local PadChest sample (for quick testing / debugging)\n",
    "    padchest_csv: str = os.path.expanduser(\n",
    "        \"~/Documents/cit/iiitdm/archive (5)/chest_x_ray_images_labels_sample.csv\"\n",
    "    )\n",
    "    padchest_images: str = os.path.expanduser(\n",
    "        \"~/Documents/cit/iiitdm/archive (5)/sample\"\n",
    "    )\n",
    "\n",
    "    # Option B: Full PadChest from Google Drive (160K+ images)\n",
    "    # Images live in numbered sub-folders: images/0/, images/1/, ..., images/37/\n",
    "    # The CSV column \"ImageDir\" maps each row to its sub-folder number.\n",
    "    use_full_padchest: bool = True   # <<< SET True FOR FULL DATASET\n",
    "\n",
    "    # --- Auto-detected Google Drive path (set by Cell 1) ---\n",
    "    # GDRIVE_PADCHEST is set automatically in Cell 1:\n",
    "    #   - Colab:   /content/drive/MyDrive/Padchest\n",
    "    #   - Windows: G:/My Drive/Padchest  (or whichever drive letter)\n",
    "    #   - macOS:   ~/Library/CloudStorage/GoogleDrive-.../My Drive/Padchest\n",
    "    #   - Linux:   ~/Google Drive/My Drive/Padchest\n",
    "    #\n",
    "    # If auto-detection failed, override manually below:\n",
    "    gdrive_padchest_csv: str = \"\"    # Will be set below\n",
    "    gdrive_padchest_images: str = \"\" # Will be set below\n",
    "\n",
    "    # ---- Manual Override (uncomment ONE if auto-detection didn't work) ----\n",
    "    # Colab:\n",
    "    #   gdrive_padchest_csv = \"/content/drive/MyDrive/Padchest/PADCHEST_chest_x_ray_images_labels_160K.csv\"\n",
    "    #   gdrive_padchest_images = \"/content/drive/MyDrive/Padchest/images\"\n",
    "    #\n",
    "    # Windows Google Drive for Desktop (check your drive letter in \"This PC\"):\n",
    "    #   gdrive_padchest_csv = \"G:/My Drive/Padchest/PADCHEST_chest_x_ray_images_labels_160K.csv\"\n",
    "    #   gdrive_padchest_images = \"G:/My Drive/Padchest/images\"\n",
    "    #\n",
    "    # macOS Google Drive for Desktop:\n",
    "    #   gdrive_padchest_csv = str(pathlib.Path.home() / \"Library/CloudStorage/GoogleDrive-YOUR_EMAIL/My Drive/Padchest/PADCHEST_chest_x_ray_images_labels_160K.csv\")\n",
    "    #   gdrive_padchest_images = str(pathlib.Path.home() / \"Library/CloudStorage/GoogleDrive-YOUR_EMAIL/My Drive/Padchest/images\")\n",
    "    #\n",
    "    # Local download (last resort — needs ~300 GB):\n",
    "    #   gdrive_padchest_csv = \"C:/Datasets/Padchest/PADCHEST_chest_x_ray_images_labels_160K.csv\"\n",
    "    #   gdrive_padchest_images = \"C:/Datasets/Padchest/images\"\n",
    "\n",
    "    # ---- Output ----\n",
    "    output_dir: str = \"./explainmyxray-v2-medgemma-padchest\"\n",
    "    hub_model_id: str = \"explainmyxray-v2-medgemma-4b-padchest\"\n",
    "\n",
    "    # ---- QLoRA ----\n",
    "    lora_r: int = 32               # ↑ from 16 — more capacity for 95%+ accuracy\n",
    "    lora_alpha: int = 64            # ↑ alpha = 2×r for stronger adaptation\n",
    "    lora_dropout: float = 0.05\n",
    "    load_in_4bit: bool = True\n",
    "    bnb_4bit_quant_type: str = \"nf4\"\n",
    "    bnb_4bit_use_double_quant: bool = True\n",
    "\n",
    "    # ---- Training (optimized for 12 GB VRAM + 95%+ accuracy) ----\n",
    "    num_train_epochs: int = 5       # ↑ from 3 — more passes for convergence\n",
    "    per_device_train_batch_size: int = 1   # Conservative for 12 GB\n",
    "    per_device_eval_batch_size: int = 1\n",
    "    gradient_accumulation_steps: int = 32  # ↑ effective batch = 32 for stability\n",
    "    learning_rate: float = 1e-4     # ↓ from 2e-4 — gentler for pretrained model\n",
    "    warmup_ratio: float = 0.05     # ↑ from 0.03 — longer warmup\n",
    "    max_grad_norm: float = 0.3\n",
    "    lr_scheduler_type: str = \"cosine\"\n",
    "    logging_steps: int = 10\n",
    "    eval_steps: int = 100          # ↑ from 50 — less overhead on 160K dataset\n",
    "    save_steps: int = 200          # ↑ from 100\n",
    "    max_seq_length: int = 512\n",
    "\n",
    "    # ---- Data ----\n",
    "    train_ratio: float = 0.90      # ↑ from 0.85 — more training data\n",
    "    val_ratio: float = 0.05\n",
    "    test_ratio: float = 0.05\n",
    "    max_samples: int = 0           # 0 = use all; set >0 for quick test runs\n",
    "\n",
    "    # ---- Curriculum Learning ----\n",
    "    use_curriculum: bool = True\n",
    "\n",
    "    # ---- Accuracy Target ----\n",
    "    target_accuracy: float = 0.95  # 95% minimum\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "# ---- Auto-set Google Drive paths from Cell 1 detection ----\n",
    "if GDRIVE_PADCHEST is not None and cfg.use_full_padchest:\n",
    "    cfg.gdrive_padchest_csv = os.path.join(GDRIVE_PADCHEST, \"PADCHEST_chest_x_ray_images_labels_160K.csv\")\n",
    "    cfg.gdrive_padchest_images = os.path.join(GDRIVE_PADCHEST, \"images\")\n",
    "    print(f\"✅ Auto-detected Google Drive paths:\")\n",
    "    print(f\"   CSV:    {cfg.gdrive_padchest_csv}\")\n",
    "    print(f\"   Images: {cfg.gdrive_padchest_images}\")\n",
    "elif cfg.use_full_padchest and not cfg.gdrive_padchest_csv:\n",
    "    print(\"⚠️  use_full_padchest=True but no Google Drive path detected!\")\n",
    "    print(\"   → Install Google Drive for Desktop and restart, OR\")\n",
    "    print(\"   → Set paths manually above in the Config class, e.g.:\")\n",
    "    print('     gdrive_padchest_csv = \"G:/My Drive/Padchest/PADCHEST_chest_x_ray_images_labels_160K.csv\"')\n",
    "    print('     gdrive_padchest_images = \"G:/My Drive/Padchest/images\"')\n",
    "\n",
    "# Validate paths\n",
    "if cfg.use_full_padchest:\n",
    "    csv_path = cfg.gdrive_padchest_csv\n",
    "    img_dir = cfg.gdrive_padchest_images\n",
    "    if csv_path and os.path.isfile(csv_path):\n",
    "        print(f\"✅ CSV file verified: {csv_path}\")\n",
    "    elif csv_path:\n",
    "        print(f\"⚠️  CSV file not found at: {csv_path}\")\n",
    "        print(\"   Check that Google Drive for Desktop is running and signed in.\")\n",
    "    if img_dir and os.path.isdir(img_dir):\n",
    "        # Count available sub-folders\n",
    "        subfolders = [d for d in os.listdir(img_dir) if os.path.isdir(os.path.join(img_dir, d))]\n",
    "        print(f\"✅ Image directory verified: {img_dir} ({len(subfolders)} sub-folders)\")\n",
    "    elif img_dir:\n",
    "        print(f\"⚠️  Image directory not found at: {img_dir}\")\n",
    "    print(\"MODE: Full PadChest dataset via Google Drive (streaming, no local download)\")\n",
    "else:\n",
    "    csv_path = cfg.padchest_csv\n",
    "    img_dir = cfg.padchest_images\n",
    "    print(\"MODE: Local PadChest sample (testing only)\")\n",
    "\n",
    "print(f\"\\nDataset CSV: {csv_path}\")\n",
    "print(f\"Image dir:   {img_dir}\")\n",
    "if cfg.use_full_padchest:\n",
    "    print(f\"Image sub-folders: {img_dir}/0/ ... {img_dir}/37/\")\n",
    "print(f\"Effective batch size: {cfg.per_device_train_batch_size * cfg.gradient_accumulation_steps}\")\n",
    "print(f\"LoRA rank: r={cfg.lora_r}, alpha={cfg.lora_alpha}\")\n",
    "print(f\"Target accuracy: {cfg.target_accuracy*100:.0f}%\")\n",
    "print(f\"Output: {cfg.output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366940d0",
   "metadata": {},
   "source": [
    "## Phase 3 — PadChest Dataset Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386366a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 6: Load PadChest CSV & Parse Labels\n",
    "# PadChest has labels in English, reports in Spanish.\n",
    "# We use the English labels + localizations for training.\n",
    "# ============================================================\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"Raw dataset: {len(df)} rows\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "\n",
    "def safe_parse_list(val):\n",
    "    \"\"\"Parse string representation of list, handling edge cases.\"\"\"\n",
    "    if pd.isna(val) or val in [\"\", \"[]\", \"nan\"]:\n",
    "        return []\n",
    "    try:\n",
    "        parsed = ast.literal_eval(val)\n",
    "        if isinstance(parsed, list):\n",
    "            # Flatten nested lists\n",
    "            flat = []\n",
    "            for item in parsed:\n",
    "                if isinstance(item, list):\n",
    "                    flat.extend(item)\n",
    "                else:\n",
    "                    flat.append(str(item).strip())\n",
    "            return flat\n",
    "        return [str(parsed).strip()]\n",
    "    except (ValueError, SyntaxError):\n",
    "        return [str(val).strip()]\n",
    "\n",
    "\n",
    "# Parse label columns\n",
    "df[\"labels_parsed\"] = df[\"Labels\"].apply(safe_parse_list)\n",
    "df[\"localizations_parsed\"] = df[\"Localizations\"].apply(safe_parse_list)\n",
    "df[\"labels_locs_parsed\"] = df[\"LabelsLocalizationsBySentence\"].apply(safe_parse_list)\n",
    "\n",
    "# Separate findings from location prefixes\n",
    "def split_findings_locations(items):\n",
    "    \"\"\"Split LabelsLocalizationsBySentence into findings and locations.\"\"\"\n",
    "    findings = []\n",
    "    locations = []\n",
    "    for item in items:\n",
    "        item_clean = item.strip()\n",
    "        if item_clean.startswith(\"loc \"):\n",
    "            locations.append(item_clean.replace(\"loc \", \"\"))\n",
    "        elif item_clean not in [\"exclude\", \"\"]:\n",
    "            findings.append(item_clean)\n",
    "    return findings, locations\n",
    "\n",
    "\n",
    "df[\"findings\"], df[\"locations\"] = zip(\n",
    "    *df[\"labels_locs_parsed\"].apply(split_findings_locations)\n",
    ")\n",
    "\n",
    "# Count findings per image\n",
    "df[\"num_findings\"] = df[\"findings\"].apply(len)\n",
    "df[\"num_locations\"] = df[\"locations\"].apply(len)\n",
    "\n",
    "print(f\"\\nParsed {len(df)} images\")\n",
    "print(f\"Finding count range: {df['num_findings'].min()}-{df['num_findings'].max()}\")\n",
    "print(f\"Location count range: {df['num_locations'].min()}-{df['num_locations'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2990f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 7: Filter Valid Images & Explore\n",
    "# ============================================================\n",
    "\n",
    "def resolve_image_path(row) -> str:\n",
    "    \"\"\"Resolve full image path. Handles numbered sub-folder structure.\n",
    "    Full PadChest: images/<ImageDir>/<ImageID>  (e.g. images/28/xxx.png)\n",
    "    Sample:        sample/<ImageID>\n",
    "    \"\"\"\n",
    "    img_name = row[\"ImageID\"]\n",
    "    if cfg.use_full_padchest:\n",
    "        # ImageDir is the numbered sub-folder (0-37)\n",
    "        img_dir_num = row.get(\"ImageDir\")\n",
    "        if pd.notna(img_dir_num):\n",
    "            return os.path.join(img_dir, str(int(img_dir_num)), img_name)\n",
    "        # Fallback: search all sub-folders\n",
    "        for sub in range(38):\n",
    "            candidate = os.path.join(img_dir, str(sub), img_name)\n",
    "            if os.path.exists(candidate):\n",
    "                return candidate\n",
    "    return os.path.join(img_dir, img_name)\n",
    "\n",
    "\n",
    "def check_image_exists(row) -> bool:\n",
    "    \"\"\"Check if image file exists.\"\"\"\n",
    "    return os.path.exists(resolve_image_path(row))\n",
    "\n",
    "\n",
    "print(\"Scanning for images on disk...\")\n",
    "df[\"image_path\"] = df.apply(resolve_image_path, axis=1)\n",
    "df[\"image_exists\"] = df[\"image_path\"].apply(os.path.exists)\n",
    "df_valid = df[df[\"image_exists\"]].copy()\n",
    "print(f\"Images found on disk: {len(df_valid)} / {len(df)}\")\n",
    "\n",
    "if cfg.use_full_padchest:\n",
    "    # Show per-folder breakdown\n",
    "    folder_counts = df_valid[\"image_path\"].apply(\n",
    "        lambda p: os.path.basename(os.path.dirname(p))\n",
    "    ).value_counts().sort_index()\n",
    "    print(f\"\\nImages per sub-folder (top 10):\")\n",
    "    for folder, count in folder_counts.head(10).items():\n",
    "        print(f\"  folder {folder}/: {count}\")\n",
    "    print(f\"  ... ({len(folder_counts)} folders total)\")\n",
    "\n",
    "if len(df_valid) == 0:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No images found in {img_dir}. \"\n",
    "        f\"Check your paths in Config. \"\n",
    "        f\"For full PadChest, ensure Drive is mounted and sub-folders 0-37 exist.\"\n",
    "    )\n",
    "\n",
    "# Apply max_samples limit if set\n",
    "if cfg.max_samples > 0:\n",
    "    df_valid = df_valid.sample(n=min(cfg.max_samples, len(df_valid)), random_state=SEED)\n",
    "    print(f\"Limited to {len(df_valid)} samples\")\n",
    "\n",
    "# Show label distribution\n",
    "all_findings = [f for findings in df_valid[\"findings\"] for f in findings]\n",
    "finding_counts = Counter(all_findings)\n",
    "print(f\"\\nUnique findings: {len(finding_counts)}\")\n",
    "print(\"\\nTop 20 findings:\")\n",
    "for finding, count in finding_counts.most_common(20):\n",
    "    pct = 100 * count / len(df_valid)\n",
    "    print(f\"  {finding}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "# Show location distribution\n",
    "all_locations = [loc for locs in df_valid[\"locations\"] for loc in locs]\n",
    "loc_counts = Counter(all_locations)\n",
    "print(f\"\\nUnique locations: {len(loc_counts)}\")\n",
    "print(\"\\nTop 15 locations:\")\n",
    "for loc, count in loc_counts.most_common(15):\n",
    "    print(f\"  {loc}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772f2969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 8: Build Structured Prompts (Multi-Task)\n",
    "# Each training example produces a structured report with:\n",
    "#   - Classification (findings list)\n",
    "#   - Localization (anatomical regions per finding)\n",
    "#   - Severity estimate\n",
    "#   - Brief narrative report\n",
    "# ============================================================\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are an expert radiologist AI assistant specialized in chest X-ray interpretation. \"\n",
    "    \"Analyze the provided chest X-ray image and produce a structured radiology report. \"\n",
    "    \"For each finding, specify the anatomical location. \"\n",
    "    \"Be precise, systematic, and clinically actionable.\"\n",
    ")\n",
    "\n",
    "\n",
    "def build_user_prompt(view_position: str = \"PA\") -> str:\n",
    "    \"\"\"Build the user prompt with clinical context.\"\"\"\n",
    "    view_str = view_position if pd.notna(view_position) and view_position else \"unknown\"\n",
    "    return (\n",
    "        f\"Analyze this chest X-ray (view: {view_str}).\\n\"\n",
    "        f\"Provide a structured report with:\\n\"\n",
    "        f\"1. FINDINGS: List each radiographic finding\\n\"\n",
    "        f\"2. LOCATIONS: Anatomical location for each finding\\n\"\n",
    "        f\"3. IMPRESSION: Brief clinical summary\"\n",
    "    )\n",
    "\n",
    "\n",
    "def build_assistant_response(findings: list, locations: list) -> str:\n",
    "    \"\"\"Build structured assistant response from PadChest annotations.\"\"\"\n",
    "    # Deduplicate\n",
    "    findings_unique = list(dict.fromkeys(findings))\n",
    "    locations_unique = list(dict.fromkeys(locations))\n",
    "\n",
    "    # Build findings section\n",
    "    if not findings_unique or findings_unique == [\"normal\"]:\n",
    "        findings_str = \"No significant abnormalities detected.\"\n",
    "        impression = \"Normal chest X-ray. No acute cardiopulmonary disease.\"\n",
    "    else:\n",
    "        # Remove 'normal' and 'unchanged' from findings list\n",
    "        abnormal = [f for f in findings_unique if f not in [\"normal\", \"unchanged\", \"exclude\"]]\n",
    "        if not abnormal:\n",
    "            findings_str = \"No significant abnormalities detected.\"\n",
    "            impression = \"Normal chest X-ray. No acute cardiopulmonary disease.\"\n",
    "        else:\n",
    "            # Pair findings with locations where possible\n",
    "            finding_lines = []\n",
    "            for f in abnormal:\n",
    "                # Try to match locations to this finding\n",
    "                relevant_locs = [loc for loc in locations_unique if loc]\n",
    "                if relevant_locs:\n",
    "                    loc_str = \", \".join(relevant_locs[:3])  # Limit locations\n",
    "                    finding_lines.append(f\"- {f} ({loc_str})\")\n",
    "                else:\n",
    "                    finding_lines.append(f\"- {f}\")\n",
    "            findings_str = \"\\n\".join(finding_lines)\n",
    "\n",
    "            # Generate impression\n",
    "            if len(abnormal) == 1:\n",
    "                impression = f\"{abnormal[0].capitalize()} identified. Clinical correlation recommended.\"\n",
    "            else:\n",
    "                conditions = \", \".join(abnormal[:3])\n",
    "                impression = f\"Multiple findings: {conditions}. Clinical correlation recommended.\"\n",
    "\n",
    "    response = f\"FINDINGS:\\n{findings_str}\\n\\n\"\n",
    "    if locations_unique:\n",
    "        response += f\"LOCATIONS: {', '.join(locations_unique)}\\n\\n\"\n",
    "    response += f\"IMPRESSION:\\n{impression}\"\n",
    "    return response\n",
    "\n",
    "\n",
    "# Test with a sample\n",
    "sample_row = df_valid.iloc[0]\n",
    "print(\"=== Sample Prompt ===\")\n",
    "print(f\"User: {build_user_prompt(sample_row.get('Projection', 'PA'))}\")\n",
    "print(f\"\\nAssistant: {build_assistant_response(sample_row['findings'], sample_row['locations'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5ebd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 9: Curriculum Learning — Sort by Difficulty\n",
    "# Easy: normal/single finding → Hard: multi-finding with locs\n",
    "# ============================================================\n",
    "\n",
    "def compute_difficulty(row) -> int:\n",
    "    \"\"\"Score difficulty: 0=easy, higher=harder.\"\"\"\n",
    "    score = 0\n",
    "    findings = row[\"findings\"]\n",
    "    locations = row[\"locations\"]\n",
    "\n",
    "    # Number of findings (more = harder)\n",
    "    n_findings = len([f for f in findings if f not in [\"normal\", \"unchanged\"]])\n",
    "    score += n_findings * 2\n",
    "\n",
    "    # Has localization data (adds complexity)\n",
    "    if locations:\n",
    "        score += len(locations)\n",
    "\n",
    "    # Rare findings are harder\n",
    "    for f in findings:\n",
    "        if f in finding_counts and finding_counts[f] <= 2:\n",
    "            score += 3  # Rare finding penalty\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "df_valid[\"difficulty\"] = df_valid.apply(compute_difficulty, axis=1)\n",
    "\n",
    "if cfg.use_curriculum:\n",
    "    df_valid = df_valid.sort_values(\"difficulty\").reset_index(drop=True)\n",
    "    print(\"Curriculum learning ENABLED: samples sorted easy → hard\")\n",
    "else:\n",
    "    df_valid = df_valid.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "    print(\"Curriculum learning DISABLED: random order\")\n",
    "\n",
    "print(f\"\\nDifficulty distribution:\")\n",
    "print(df_valid[\"difficulty\"].describe())\n",
    "print(f\"\\nEasiest: {df_valid.iloc[0]['findings']}\")\n",
    "print(f\"Hardest:  {df_valid.iloc[-1]['findings']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3871798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 10: Convert to HuggingFace Dataset with Chat Template\n",
    "# MedGemma uses the chat template format:\n",
    "#   [{\"role\": \"system\", ...}, {\"role\": \"user\", ...}, {\"role\": \"assistant\", ...}]\n",
    "# ============================================================\n",
    "\n",
    "def row_to_example(row) -> dict | None:\n",
    "    \"\"\"Convert a DataFrame row to a training example.\n",
    "    Returns None if image cannot be loaded.\"\"\"\n",
    "    img_name = row[\"ImageID\"]\n",
    "    img_path = row[\"image_path\"]  # Pre-resolved in Cell 7\n",
    "\n",
    "    # Load image\n",
    "    try:\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {img_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    view = row.get(\"Projection\", \"PA\")\n",
    "    findings = row[\"findings\"]\n",
    "    locations = row[\"locations\"]\n",
    "\n",
    "    # Build chat messages\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": SYSTEM_PROMPT}],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": build_user_prompt(view)},\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": build_assistant_response(findings, locations)},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"image\": image,\n",
    "        \"messages\": messages,\n",
    "        \"findings\": findings,\n",
    "        \"locations\": locations,\n",
    "        \"image_id\": img_name,\n",
    "    }\n",
    "\n",
    "\n",
    "# Convert all rows\n",
    "print(\"Converting dataset to training format...\")\n",
    "examples = []\n",
    "failed = 0\n",
    "for idx, row in df_valid.iterrows():\n",
    "    ex = row_to_example(row)\n",
    "    if ex is not None:\n",
    "        examples.append(ex)\n",
    "    else:\n",
    "        failed += 1\n",
    "\n",
    "print(f\"Successfully converted {len(examples)} / {len(df_valid)} images\")\n",
    "if failed > 0:\n",
    "    print(f\"  ({failed} images failed to load)\")\n",
    "\n",
    "# Show a sample\n",
    "if examples:\n",
    "    sample = examples[0]\n",
    "    print(f\"\\nSample image: {sample['image_id']}\")\n",
    "    print(f\"Findings: {sample['findings']}\")\n",
    "    print(f\"Locations: {sample['locations']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392a9d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 11: Train / Validation / Test Split\n",
    "# Stratified if possible, otherwise random.\n",
    "# ============================================================\n",
    "\n",
    "n = len(examples)\n",
    "n_train = int(n * cfg.train_ratio)\n",
    "n_val = int(n * cfg.val_ratio)\n",
    "n_test = n - n_train - n_val\n",
    "\n",
    "# If curriculum learning is on, data is already sorted easy→hard.\n",
    "# We keep this order for training but shuffle val/test.\n",
    "train_examples = examples[:n_train]\n",
    "val_examples = examples[n_train:n_train + n_val]\n",
    "test_examples = examples[n_train + n_val:]\n",
    "\n",
    "# Shuffle val/test for fair evaluation\n",
    "random.shuffle(val_examples)\n",
    "random.shuffle(test_examples)\n",
    "\n",
    "def examples_to_dataset(exs):\n",
    "    \"\"\"Convert list of dicts to HuggingFace Dataset.\"\"\"\n",
    "    return Dataset.from_dict({\n",
    "        \"image\": [e[\"image\"] for e in exs],\n",
    "        \"messages\": [e[\"messages\"] for e in exs],\n",
    "        \"findings\": [e[\"findings\"] for e in exs],\n",
    "        \"locations\": [e[\"locations\"] for e in exs],\n",
    "        \"image_id\": [e[\"image_id\"] for e in exs],\n",
    "    })\n",
    "\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": examples_to_dataset(train_examples),\n",
    "    \"validation\": examples_to_dataset(val_examples),\n",
    "    \"test\": examples_to_dataset(test_examples),\n",
    "})\n",
    "\n",
    "print(f\"Dataset splits:\")\n",
    "for split, ds in dataset.items():\n",
    "    print(f\"  {split}: {len(ds)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0531cdd6",
   "metadata": {},
   "source": [
    "## Phase 4 — Model Loading (MedGemma-4B QLoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe80df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 12: Quantization Config + Load Model\n",
    "# 4-bit NF4 QLoRA — fits MedGemma-4B in ~2.5 GB base VRAM\n",
    "# Combined with paged_adamw_8bit optimizer → total ~8 GB of 12 GB\n",
    "# ============================================================\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=cfg.load_in_4bit,\n",
    "    bnb_4bit_use_double_quant=cfg.bnb_4bit_use_double_quant,\n",
    "    bnb_4bit_quant_type=cfg.bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_storage=torch.bfloat16,\n",
    ")\n",
    "\n",
    "print(\"Loading MedGemma-4B-it with 4-bit quantization...\")\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    cfg.model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    attn_implementation=\"eager\",  # Required for MedGemma\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(cfg.model_id)\n",
    "processor.tokenizer.padding_side = \"right\"  # Right-pad for training\n",
    "\n",
    "# Memory check\n",
    "allocated = torch.cuda.memory_allocated() / 1e9\n",
    "reserved = torch.cuda.memory_reserved() / 1e9\n",
    "print(f\"\\nModel loaded.\")\n",
    "print(f\"VRAM allocated: {allocated:.2f} GB\")\n",
    "print(f\"VRAM reserved:  {reserved:.2f} GB\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f20ee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 13: LoRA Configuration\n",
    "# Following official MedGemma fine-tuning notebook:\n",
    "#   - target_modules=\"all-linear\" (not just attention)\n",
    "#   - modules_to_save=[\"lm_head\", \"embed_tokens\"]\n",
    "#   - r=32, alpha=64 for higher capacity → 95%+ accuracy\n",
    "# ============================================================\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=cfg.lora_r,\n",
    "    lora_alpha=cfg.lora_alpha,\n",
    "    lora_dropout=cfg.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\n",
    "        \"lm_head\",\n",
    "        \"embed_tokens\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(f\"LoRA config:\")\n",
    "print(f\"  r={peft_config.r}, alpha={peft_config.lora_alpha}\")\n",
    "print(f\"  dropout={peft_config.lora_dropout}\")\n",
    "print(f\"  target: {peft_config.target_modules}\")\n",
    "print(f\"  modules_to_save: {peft_config.modules_to_save}\")\n",
    "print(f\"  Estimated trainable params: ~{cfg.lora_r * 2 * 4096 * 100 / 1e6:.0f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9052fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 14: Custom Data Collator\n",
    "# From official MedGemma fine-tuning notebook.\n",
    "# Applies chat template, tokenizes, masks labels properly.\n",
    "# ============================================================\n",
    "\n",
    "def collate_fn(examples: list[dict[str, Any]]):\n",
    "    \"\"\"Process examples into model-ready batches.\n",
    "    \n",
    "    Applies the chat template, tokenizes text + images,\n",
    "    and creates labels with proper masking.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    images = []\n",
    "\n",
    "    for example in examples:\n",
    "        img = example[\"image\"]\n",
    "        if not isinstance(img, Image.Image):\n",
    "            img = Image.open(img).convert(\"RGB\")\n",
    "        images.append([img.convert(\"RGB\")])\n",
    "\n",
    "        # Apply chat template to get formatted text\n",
    "        text = processor.apply_chat_template(\n",
    "            example[\"messages\"],\n",
    "            add_generation_prompt=False,\n",
    "            tokenize=False,\n",
    "        ).strip()\n",
    "        texts.append(text)\n",
    "\n",
    "    # Tokenize and process images\n",
    "    batch = processor(\n",
    "        text=texts,\n",
    "        images=images,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=cfg.max_seq_length,\n",
    "    )\n",
    "\n",
    "    # Create labels — mask padding and image tokens\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "\n",
    "    # Get special token IDs to mask\n",
    "    pad_token_id = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # Mask image tokens (BOI token)\n",
    "    boi_token = processor.tokenizer.special_tokens_map.get(\"boi_token\")\n",
    "    if boi_token:\n",
    "        image_token_id = processor.tokenizer.convert_tokens_to_ids(boi_token)\n",
    "        labels[labels == image_token_id] = -100\n",
    "\n",
    "    # Mask padding tokens\n",
    "    if pad_token_id is not None:\n",
    "        labels[labels == pad_token_id] = -100\n",
    "\n",
    "    # Mask special image placeholder (262144)\n",
    "    labels[labels == 262144] = -100\n",
    "\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n",
    "\n",
    "\n",
    "print(\"Custom data collator defined.\")\n",
    "\n",
    "# Quick test with first example\n",
    "test_batch = collate_fn([dataset[\"train\"][0]])\n",
    "print(f\"Test batch keys: {list(test_batch.keys())}\")\n",
    "print(f\"Input shape: {test_batch['input_ids'].shape}\")\n",
    "print(f\"Labels shape: {test_batch['labels'].shape}\")\n",
    "n_masked = (test_batch['labels'] == -100).sum().item()\n",
    "n_total = test_batch['labels'].numel()\n",
    "print(f\"Masked tokens: {n_masked}/{n_total} ({100*n_masked/n_total:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12035130",
   "metadata": {},
   "source": [
    "## Phase 5 — Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00911294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 15: SFTConfig — Training Arguments\n",
    "# Tuned for 95%+ accuracy on RTX 4080 Laptop 12 GB VRAM.\n",
    "# Uses TRL SFTConfig (not vanilla TrainingArguments).\n",
    "# ============================================================\n",
    "\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    # Output\n",
    "    output_dir=cfg.output_dir,\n",
    "\n",
    "    # Epochs & batching\n",
    "    num_train_epochs=cfg.num_train_epochs,\n",
    "    per_device_train_batch_size=cfg.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=cfg.per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=cfg.gradient_accumulation_steps,\n",
    "\n",
    "    # Memory optimization (CRITICAL for 12 GB)\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "\n",
    "    # Optimizer — 8-bit paged AdamW saves ~1 GB VRAM vs standard AdamW\n",
    "    # Uses bitsandbytes 8-bit optimizer states + CPU paging if VRAM is tight\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=cfg.learning_rate,\n",
    "    warmup_ratio=cfg.warmup_ratio,\n",
    "    max_grad_norm=cfg.max_grad_norm,\n",
    "    lr_scheduler_type=cfg.lr_scheduler_type,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    # Precision\n",
    "    bf16=True,\n",
    "\n",
    "    # Logging & evaluation\n",
    "    logging_steps=cfg.logging_steps,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=cfg.eval_steps,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=cfg.save_steps,\n",
    "    save_total_limit=5,              # ↑ keep more checkpoints\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "\n",
    "    # Reporting\n",
    "    report_to=\"tensorboard\",\n",
    "\n",
    "    # Dataset handling\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    "\n",
    "    # Data loading\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=4,        # ↑ from 2 — faster with Drive streaming\n",
    ")\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=5,       # Stop if no improvement for 5 evals\n",
    "    early_stopping_threshold=0.001,\n",
    ")\n",
    "\n",
    "print(\"Training configuration (tuned for 95%+ accuracy):\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch (per device): {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Scheduler: {training_args.lr_scheduler_type}\")\n",
    "print(f\"  Precision: bf16={training_args.bf16}\")\n",
    "print(f\"  Optimizer: {training_args.optim}\")\n",
    "print(f\"  Gradient checkpointing: {training_args.gradient_checkpointing}\")\n",
    "print(f\"  Early stopping patience: 5 eval rounds\")\n",
    "print(f\"  Target: ≥{cfg.target_accuracy*100:.0f}% accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65eeb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 16: Build SFTTrainer\n",
    "# ============================================================\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    peft_config=peft_config,\n",
    "    processing_class=processor,\n",
    "    data_collator=collate_fn,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable, total = 0, 0\n",
    "for p in model.parameters():\n",
    "    total += p.numel()\n",
    "    if p.requires_grad:\n",
    "        trainable += p.numel()\n",
    "\n",
    "print(f\"\\nTrainable parameters: {trainable:,} / {total:,} ({100 * trainable / total:.2f}%)\")\n",
    "print(f\"Trainable size: ~{trainable * 2 / 1e6:.1f} MB (bf16)\")\n",
    "n_train = len(dataset[\"train\"])\n",
    "steps_per_epoch = n_train // (cfg.per_device_train_batch_size * cfg.gradient_accumulation_steps)\n",
    "total_steps = steps_per_epoch * cfg.num_train_epochs\n",
    "print(f\"\\nEstimated steps: {steps_per_epoch}/epoch × {cfg.num_train_epochs} epochs = {total_steps} total\")\n",
    "print(f\"Eval every {cfg.eval_steps} steps → ~{total_steps // cfg.eval_steps} evaluations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368d3e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 17: Train!\n",
    "# ============================================================\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"VRAM before training: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Training complete!\")\n",
    "print(f\"Final training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"Total steps: {train_result.global_step}\")\n",
    "print(f\"Runtime: {train_result.metrics['train_runtime']:.0f}s\")\n",
    "print(f\"Samples/sec: {train_result.metrics['train_samples_per_second']:.2f}\")\n",
    "print(f\"Peak VRAM: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef69a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 18: Save Model\n",
    "# Saves LoRA adapters (small) — not the full 4B model.\n",
    "# ============================================================\n",
    "\n",
    "trainer.save_model(cfg.output_dir)\n",
    "processor.save_pretrained(cfg.output_dir)\n",
    "\n",
    "# Report adapter size\n",
    "adapter_size = sum(\n",
    "    os.path.getsize(os.path.join(cfg.output_dir, f))\n",
    "    for f in os.listdir(cfg.output_dir)\n",
    "    if os.path.isfile(os.path.join(cfg.output_dir, f))\n",
    ") / 1e6\n",
    "\n",
    "print(f\"Model saved to: {cfg.output_dir}\")\n",
    "print(f\"Adapter size: {adapter_size:.1f} MB\")\n",
    "print(f\"\\nTo push to Hub, run:\")\n",
    "print(f\"  trainer.push_to_hub('{cfg.hub_model_id}')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8828bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 19: Cleanup Memory for Evaluation\n",
    "# ============================================================\n",
    "\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Memory freed. VRAM: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b22da6",
   "metadata": {},
   "source": [
    "## Phase 6 — Evaluation & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7468d305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 20: Load Fine-Tuned Model for Inference\n",
    "# Uses HuggingFace pipeline API for easy batch inference.\n",
    "# ============================================================\n",
    "\n",
    "# Load fine-tuned model with LoRA adapter\n",
    "ft_pipe = pipeline(\n",
    "    \"image-text-to-text\",\n",
    "    model=cfg.output_dir,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Deterministic generation\n",
    "ft_pipe.model.generation_config.do_sample = False\n",
    "ft_pipe.model.generation_config.max_new_tokens = 256\n",
    "\n",
    "# Use left padding for inference\n",
    "ft_pipe.tokenizer.padding_side = \"left\"\n",
    "\n",
    "print(\"Fine-tuned model loaded for inference.\")\n",
    "print(f\"VRAM: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b3b969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 21: Run Inference on Test Set\n",
    "# ============================================================\n",
    "\n",
    "def run_inference(pipe, examples, max_samples=None):\n",
    "    \"\"\"Run inference and collect predictions.\"\"\"\n",
    "    results = []\n",
    "    n = min(len(examples), max_samples) if max_samples else len(examples)\n",
    "\n",
    "    for i in range(n):\n",
    "        ex = examples[i]\n",
    "        img = ex[\"image\"]\n",
    "\n",
    "        # Build inference messages (no assistant response)\n",
    "        view = \"PA\"  # Default\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": SYSTEM_PROMPT}],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": build_user_prompt(view)},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            output = pipe(\n",
    "                text=messages,\n",
    "                images=[img],\n",
    "                return_full_text=False,\n",
    "            )\n",
    "            prediction = output[0][\"generated_text\"]\n",
    "        except Exception as e:\n",
    "            prediction = f\"ERROR: {e}\"\n",
    "\n",
    "        results.append({\n",
    "            \"image_id\": ex[\"image_id\"],\n",
    "            \"ground_truth_findings\": ex[\"findings\"],\n",
    "            \"ground_truth_locations\": ex[\"locations\"],\n",
    "            \"prediction\": prediction,\n",
    "        })\n",
    "\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"  Processed {i + 1}/{n}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "print(f\"Running inference on {len(dataset['test'])} test examples...\")\n",
    "test_results = run_inference(ft_pipe, dataset[\"test\"])\n",
    "print(f\"\\nInference complete: {len(test_results)} predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033e1121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 22: Evaluate Predictions — Target 95%+ Accuracy\n",
    "# Parse structured predictions and compare with ground truth.\n",
    "# Uses multi-label soft matching for real-world accuracy.\n",
    "# ============================================================\n",
    "\n",
    "def extract_findings_from_prediction(prediction_text: str) -> list[str]:\n",
    "    \"\"\"Extract findings from a structured prediction.\"\"\"\n",
    "    findings = []\n",
    "    in_findings_section = False\n",
    "\n",
    "    for line in prediction_text.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"FINDINGS:\"):\n",
    "            in_findings_section = True\n",
    "            continue\n",
    "        if line.startswith((\"LOCATIONS:\", \"IMPRESSION:\")):\n",
    "            in_findings_section = False\n",
    "            continue\n",
    "        if in_findings_section and line.startswith(\"- \"):\n",
    "            # Extract finding, remove location in parentheses\n",
    "            finding = line.lstrip(\"- \").split(\"(\")[0].strip()\n",
    "            findings.append(finding.lower())\n",
    "        elif in_findings_section and \"no significant\" in line.lower():\n",
    "            findings.append(\"normal\")\n",
    "\n",
    "    return findings if findings else [\"normal\"]\n",
    "\n",
    "\n",
    "def extract_locations_from_prediction(prediction_text: str) -> list[str]:\n",
    "    \"\"\"Extract locations from a structured prediction.\"\"\"\n",
    "    for line in prediction_text.split(\"\\n\"):\n",
    "        if line.strip().startswith(\"LOCATIONS:\"):\n",
    "            locs_text = line.replace(\"LOCATIONS:\", \"\").strip()\n",
    "            return [l.strip().lower() for l in locs_text.split(\",\") if l.strip()]\n",
    "    return []\n",
    "\n",
    "\n",
    "# ---- Compute metrics ----\n",
    "exact_match = 0\n",
    "soft_match = 0            # ≥50% overlap = soft match\n",
    "total = 0\n",
    "per_finding_tp = Counter()\n",
    "per_finding_fp = Counter()\n",
    "per_finding_fn = Counter()\n",
    "\n",
    "for result in test_results:\n",
    "    gt_findings = set(f.lower().strip() for f in result[\"ground_truth_findings\"])\n",
    "    pred_findings = set(extract_findings_from_prediction(result[\"prediction\"]))\n",
    "\n",
    "    # Exact match\n",
    "    if gt_findings == pred_findings:\n",
    "        exact_match += 1\n",
    "\n",
    "    # Soft match: ≥50% of ground truth findings found in prediction\n",
    "    if gt_findings and len(gt_findings & pred_findings) / len(gt_findings) >= 0.5:\n",
    "        soft_match += 1\n",
    "\n",
    "    total += 1\n",
    "\n",
    "    # Per-finding analysis\n",
    "    for f in gt_findings & pred_findings:\n",
    "        per_finding_tp[f] += 1\n",
    "    for f in pred_findings - gt_findings:\n",
    "        per_finding_fp[f] += 1\n",
    "    for f in gt_findings - pred_findings:\n",
    "        per_finding_fn[f] += 1\n",
    "\n",
    "exact_acc = exact_match / total if total > 0 else 0\n",
    "soft_acc = soft_match / total if total > 0 else 0\n",
    "\n",
    "# Micro-averaged metrics\n",
    "total_tp = sum(per_finding_tp.values())\n",
    "total_fp = sum(per_finding_fp.values())\n",
    "total_fn = sum(per_finding_fn.values())\n",
    "micro_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "micro_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "micro_f1 = 2 * micro_precision * micro_recall / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  EVALUATION RESULTS — Target: ≥{cfg.target_accuracy*100:.0f}%\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Exact match accuracy:  {exact_match}/{total} ({exact_acc*100:.1f}%)\")\n",
    "print(f\"  Soft match accuracy:   {soft_match}/{total} ({soft_acc*100:.1f}%)\")\n",
    "print(f\"  Micro Precision:       {micro_precision:.3f}\")\n",
    "print(f\"  Micro Recall:          {micro_recall:.3f}\")\n",
    "print(f\"  Micro F1:              {micro_f1:.3f}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if soft_acc >= cfg.target_accuracy:\n",
    "    print(f\"  ✅ TARGET MET: {soft_acc*100:.1f}% ≥ {cfg.target_accuracy*100:.0f}%\")\n",
    "else:\n",
    "    gap = cfg.target_accuracy - soft_acc\n",
    "    print(f\"  ❌ TARGET NOT MET: {soft_acc*100:.1f}% < {cfg.target_accuracy*100:.0f}% (gap: {gap*100:.1f}%)\")\n",
    "    print(f\"  → Try: more epochs, lower lr, larger LoRA rank, more data\")\n",
    "\n",
    "# Per-finding breakdown\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"  PER-FINDING ANALYSIS\")\n",
    "print(f\"{'='*60}\")\n",
    "all_findings_eval = set(per_finding_tp) | set(per_finding_fp) | set(per_finding_fn)\n",
    "for finding in sorted(all_findings_eval):\n",
    "    tp = per_finding_tp[finding]\n",
    "    fp = per_finding_fp[finding]\n",
    "    fn = per_finding_fn[finding]\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    print(f\"  {finding:35s}  P={precision:.2f}  R={recall:.2f}  F1={f1:.2f}  (TP={tp} FP={fp} FN={fn})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b841b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 23: Visualize Predictions with Anatomical Overlays\n",
    "# Maps text-based localizations to approximate bounding box\n",
    "# regions on the chest X-ray.\n",
    "# ============================================================\n",
    "\n",
    "# Approximate anatomical regions as (x_start, y_start, width, height)\n",
    "# normalized to [0, 1] relative to image dimensions\n",
    "ANATOMICAL_REGIONS = {\n",
    "    \"right upper lobe\": (0.05, 0.05, 0.35, 0.30),\n",
    "    \"right middle lobe\": (0.05, 0.30, 0.35, 0.25),\n",
    "    \"right lower lobe\": (0.05, 0.50, 0.35, 0.30),\n",
    "    \"left upper lobe\": (0.55, 0.05, 0.35, 0.30),\n",
    "    \"left lower lobe\": (0.55, 0.50, 0.35, 0.30),\n",
    "    \"right\": (0.05, 0.05, 0.40, 0.80),\n",
    "    \"left\": (0.50, 0.05, 0.40, 0.80),\n",
    "    \"bilateral\": (0.05, 0.15, 0.85, 0.65),\n",
    "    \"cardiac\": (0.30, 0.30, 0.35, 0.40),\n",
    "    \"hilar\": (0.30, 0.20, 0.35, 0.30),\n",
    "    \"aortic\": (0.35, 0.05, 0.25, 0.40),\n",
    "    \"supra aortic\": (0.30, 0.00, 0.30, 0.15),\n",
    "    \"costophrenic angle\": (0.05, 0.70, 0.85, 0.20),\n",
    "    \"right costophrenic angle\": (0.05, 0.70, 0.35, 0.20),\n",
    "    \"left costophrenic angle\": (0.55, 0.70, 0.35, 0.20),\n",
    "    \"basal\": (0.05, 0.60, 0.85, 0.25),\n",
    "    \"basal bilateral\": (0.05, 0.60, 0.85, 0.25),\n",
    "    \"middle lobe\": (0.05, 0.30, 0.35, 0.25),\n",
    "    \"diaphragm\": (0.10, 0.70, 0.75, 0.15),\n",
    "    \"pleural\": (0.00, 0.10, 0.95, 0.75),\n",
    "    \"rib\": (0.00, 0.05, 0.95, 0.80),\n",
    "    \"subsegmental\": (0.10, 0.40, 0.30, 0.20),\n",
    "    \"peribronchi\": (0.25, 0.20, 0.40, 0.30),\n",
    "    \"esophageal\": (0.38, 0.15, 0.18, 0.50),\n",
    "    \"gastric chamber\": (0.40, 0.70, 0.25, 0.20),\n",
    "    \"anterior rib\": (0.00, 0.10, 0.95, 0.50),\n",
    "}\n",
    "\n",
    "FINDING_COLORS = {\n",
    "    \"cardiomegaly\": \"red\",\n",
    "    \"pleural effusion\": \"blue\",\n",
    "    \"pneumonia\": \"orange\",\n",
    "    \"nodule\": \"yellow\",\n",
    "    \"infiltrates\": \"cyan\",\n",
    "    \"consolidation\": \"magenta\",\n",
    "    \"atelectasis\": \"lime\",\n",
    "    \"pulmonary fibrosis\": \"purple\",\n",
    "    \"COPD signs\": \"pink\",\n",
    "}\n",
    "\n",
    "\n",
    "def visualize_prediction(image, findings, locations, title=\"\"):\n",
    "    \"\"\"Visualize X-ray with anatomical region overlays.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "    # Original image\n",
    "    axes[0].imshow(image, cmap=\"gray\")\n",
    "    axes[0].set_title(\"Original X-ray\", fontsize=12)\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    # Image with overlays\n",
    "    axes[1].imshow(image, cmap=\"gray\")\n",
    "    w, h = image.size\n",
    "\n",
    "    patches_list = []\n",
    "    for i, loc in enumerate(locations):\n",
    "        loc_key = loc.lower().strip()\n",
    "        if loc_key in ANATOMICAL_REGIONS:\n",
    "            rx, ry, rw, rh = ANATOMICAL_REGIONS[loc_key]\n",
    "            color = list(FINDING_COLORS.values())[i % len(FINDING_COLORS)]\n",
    "\n",
    "            rect = Rectangle(\n",
    "                (rx * w, ry * h), rw * w, rh * h,\n",
    "                linewidth=2, edgecolor=color, facecolor=color, alpha=0.15,\n",
    "            )\n",
    "            axes[1].add_patch(rect)\n",
    "            # Border\n",
    "            rect_border = Rectangle(\n",
    "                (rx * w, ry * h), rw * w, rh * h,\n",
    "                linewidth=2, edgecolor=color, facecolor=\"none\",\n",
    "            )\n",
    "            axes[1].add_patch(rect_border)\n",
    "            patches_list.append(mpatches.Patch(color=color, label=f\"{loc_key}\"))\n",
    "\n",
    "    if patches_list:\n",
    "        axes[1].legend(handles=patches_list, loc=\"lower right\", fontsize=8)\n",
    "\n",
    "    axes[1].set_title(\"Anatomical Localization\", fontsize=12)\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    # Add text info below\n",
    "    fig.suptitle(title, fontsize=14, fontweight=\"bold\")\n",
    "    findings_str = \", \".join(findings) if findings else \"normal\"\n",
    "    fig.text(0.5, 0.02, f\"Findings: {findings_str}\", ha=\"center\", fontsize=10)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize first few test predictions\n",
    "n_show = min(5, len(test_results))\n",
    "for i in range(n_show):\n",
    "    result = test_results[i]\n",
    "    img = dataset[\"test\"][i][\"image\"]\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Image: {result['image_id']}\")\n",
    "    print(f\"Ground truth: {result['ground_truth_findings']}\")\n",
    "    print(f\"Prediction:\\n{result['prediction']}\")\n",
    "\n",
    "    visualize_prediction(\n",
    "        image=img,\n",
    "        findings=result[\"ground_truth_findings\"],\n",
    "        locations=result[\"ground_truth_locations\"],\n",
    "        title=f\"Test #{i+1}: {result['image_id']}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e002176b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 24: Interactive Single-Image Inference\n",
    "# Use this for testing on new X-ray images.\n",
    "# ============================================================\n",
    "\n",
    "def predict_xray(image_path: str, view: str = \"PA\") -> str:\n",
    "    \"\"\"Run inference on a single chest X-ray image.\"\"\"\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": SYSTEM_PROMPT}],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": build_user_prompt(view)},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    output = ft_pipe(\n",
    "        text=messages,\n",
    "        images=[img],\n",
    "        return_full_text=False,\n",
    "    )\n",
    "\n",
    "    prediction = output[0][\"generated_text\"]\n",
    "\n",
    "    # Extract findings and locations from prediction text\n",
    "    pred_findings = extract_findings_from_prediction(prediction)\n",
    "    pred_locations = []\n",
    "    for line in prediction.split(\"\\n\"):\n",
    "        if line.strip().startswith(\"LOCATIONS:\"):\n",
    "            locs_text = line.replace(\"LOCATIONS:\", \"\").strip()\n",
    "            pred_locations = [l.strip() for l in locs_text.split(\",\") if l.strip()]\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(prediction)\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    visualize_prediction(\n",
    "        image=img,\n",
    "        findings=pred_findings,\n",
    "        locations=pred_locations,\n",
    "        title=f\"Prediction: {Path(image_path).name}\",\n",
    "    )\n",
    "\n",
    "    return prediction\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# predict_xray(\"/path/to/your/chest_xray.png\", view=\"PA\")\n",
    "print(\"predict_xray() function ready. Pass an image path to analyze.\")\n",
    "print('Example: predict_xray(\"/path/to/chest_xray.png\", view=\"PA\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fc3345",
   "metadata": {},
   "source": [
    "# Phase 7 — Summary & Next Steps\n",
    "\n",
    "## What v2 delivers:\n",
    "- **MedGemma-4B-it**: Medical SigLIP pre-trained on CXR → immediate domain knowledge\n",
    "- **PadChest 160K+**: Real radiologist annotations with 174 findings, 104 locations\n",
    "- **Disease localization**: Anatomical region overlays on X-ray images\n",
    "- **Structured reports**: FINDINGS → LOCATIONS → IMPRESSION format\n",
    "- **Curriculum learning**: Easy→hard sorting for better convergence\n",
    "- **95%+ accuracy target**: LoRA r=32/α=64, 5 epochs, early stopping, cosine LR\n",
    "- **12 GB VRAM optimized**: QLoRA 4-bit + gradient checkpointing + batch=1×32\n",
    "\n",
    "## Dataset: Google Drive PadChest (Streaming — No Local Download)\n",
    "```\n",
    "Google Drive → My Drive → Padchest/\n",
    "├── PADCHEST_chest_x_ray_images_labels_160K.csv\n",
    "└── images/\n",
    "    ├── 0/   (folder of .png images)\n",
    "    ├── 1/\n",
    "    ├── ...\n",
    "    └── 37/\n",
    "```\n",
    "\n",
    "**Accessed via Google Drive for Desktop** — files stream on-demand over the internet.\n",
    "No need to download the full ~1TB dataset locally. The notebook auto-detects the Drive mount.\n",
    "\n",
    "## For your friend's system (RTX 4080 Laptop 12GB, Windows):\n",
    "\n",
    "### Recommended: Google Drive for Desktop (zero local storage for dataset)\n",
    "1. Install **Google Drive for Desktop**: https://www.google.com/drive/download/\n",
    "2. Sign in with the Google account that has the PadChest dataset\n",
    "3. After install, PadChest appears at `G:/My Drive/Padchest/` (check your drive letter in \"This PC\")\n",
    "4. Clone this repo → run `install.bat` → open notebook in VS Code\n",
    "5. Run Cell 1 — it **auto-detects** Google Drive for Desktop and sets paths\n",
    "6. Run all cells sequentially — training reads images directly from Drive (streamed)\n",
    "\n",
    "### Alternative: Local download (last resort, needs ~300 GB)\n",
    "1. Download only the CSV + the image sub-folders you need (e.g., folders 0-10)\n",
    "2. Save to `C:/Datasets/Padchest/` and set paths manually in Cell 5:\n",
    "   ```python\n",
    "   cfg.gdrive_padchest_csv = \"C:/Datasets/Padchest/PADCHEST_chest_x_ray_images_labels_160K.csv\"\n",
    "   cfg.gdrive_padchest_images = \"C:/Datasets/Padchest/images\"\n",
    "   ```\n",
    "3. Keep total local dataset under **300 GB**\n",
    "\n",
    "### Troubleshooting:\n",
    "- If auto-detection fails → check drive letter in File Explorer → set manually in Cell 5\n",
    "- If Drive files load slowly → first epoch may be slower (files cache after first access)\n",
    "- If Drive disconnects → check internet + re-sign in to Google Drive for Desktop\n",
    "- For quick test: set `use_full_padchest = False` to use the included 24 sample images\n",
    "\n",
    "## If accuracy < 95%:\n",
    "- Increase `num_train_epochs` to 7-10\n",
    "- Increase `lora_r` to 64 (uses more VRAM)\n",
    "- Lower `learning_rate` to 5e-5\n",
    "- Increase `max_seq_length` to 768 if VRAM allows\n",
    "- Disable curriculum learning and try random order\n",
    "- Filter dataset to physician-labeled rows only (higher quality)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
