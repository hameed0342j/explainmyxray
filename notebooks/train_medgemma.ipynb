{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ExplainMyXray - MedGemma Training"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 1: Install"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q -U transformers peft accelerate bitsandbytes datasets\n",
                "!pip install -q pillow==10.4.0 pandas==2.2.2\n",
                "import torch\n",
                "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 2: Login"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from huggingface_hub import login\n",
                "\n",
                "# ‚ö†Ô∏è DO NOT hardcode your token here!\n",
                "# Option 1: Set HF_TOKEN in a .env file (see .env.example)\n",
                "# Option 2: Run 'huggingface-cli login' in terminal first\n",
                "# Option 3: The login() call below will prompt you interactively\n",
                "\n",
                "HF_TOKEN = os.environ.get(\"HF_TOKEN\", \"\")\n",
                "if HF_TOKEN:\n",
                "    login(token=HF_TOKEN)\n",
                "    print(\"‚úÖ Logged in via HF_TOKEN environment variable\")\n",
                "else:\n",
                "    login()  # Interactive login\n",
                "    print(\"‚úÖ Logged in interactively\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 3: Config"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import BitsAndBytesConfig\n",
                "from peft import LoraConfig, TaskType\n",
                "\n",
                "MODEL_ID = \"google/paligemma-3b-pt-224\"\n",
                "OUTPUT_DIR = \"./medgemma_lora_adapters\"\n",
                "\n",
                "BNB_CONFIG = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
                "    bnb_4bit_use_double_quant=True,\n",
                ")\n",
                "\n",
                "LORA_CONFIG = LoraConfig(\n",
                "    r=16, lora_alpha=32,\n",
                "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
                "    lora_dropout=0.05, bias=\"none\", task_type=TaskType.CAUSAL_LM,\n",
                ")\n",
                "\n",
                "BATCH_SIZE, GRADIENT_ACCUMULATION = 2, 4\n",
                "LEARNING_RATE, NUM_EPOCHS, MAX_LENGTH = 2e-4, 3, 512\n",
                "print(\"Config ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 4: Load Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoProcessor, PaliGemmaForConditionalGeneration\n",
                "from peft import get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
                "import os\n",
                "\n",
                "# ========== CHECKPOINT FOR RESUMING (with fallback) ==========\n",
                "CHECKPOINT_PRIMARY = \"/content/drive/MyDrive/ExplainMyXray_Models/interrupted_checkpoint\"\n",
                "CHECKPOINT_FALLBACK = \"/content/drive/MyDrive/medgemma_advanced_lora/checkpoint-250\"\n",
                "\n",
                "# Try primary first, then fallback\n",
                "if os.path.exists(CHECKPOINT_PRIMARY):\n",
                "    CHECKPOINT_PATH = CHECKPOINT_PRIMARY\n",
                "    print(f\"‚úÖ Found primary checkpoint: {CHECKPOINT_PATH}\")\n",
                "elif os.path.exists(CHECKPOINT_FALLBACK):\n",
                "    CHECKPOINT_PATH = CHECKPOINT_FALLBACK\n",
                "    print(f\"‚úÖ Found fallback checkpoint: {CHECKPOINT_PATH}\")\n",
                "else:\n",
                "    CHECKPOINT_PATH = None\n",
                "    print(\"‚ö†Ô∏è No checkpoint found - will start fresh\")\n",
                "# ==============================================================\n",
                "\n",
                "print(\"Loading processor...\")\n",
                "processor = AutoProcessor.from_pretrained(MODEL_ID, token=HF_TOKEN)\n",
                "\n",
                "print(\"Loading model in 4-bit (2-3 min)...\")\n",
                "model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
                "    MODEL_ID,\n",
                "    quantization_config=BNB_CONFIG,\n",
                "    token=HF_TOKEN,\n",
                ")\n",
                "\n",
                "model = prepare_model_for_kbit_training(model)\n",
                "\n",
                "if CHECKPOINT_PATH:\n",
                "    print(f\"üîÑ Loading LoRA adapters from checkpoint: {CHECKPOINT_PATH}\")\n",
                "    try:\n",
                "        model = PeftModel.from_pretrained(model, CHECKPOINT_PATH, is_trainable=True)\n",
                "        print(\"‚úÖ Checkpoint adapters loaded successfully!\")\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Failed to load checkpoint: {e}\")\n",
                "        print(\"üÜï Falling back to fresh LoRA adapters\")\n",
                "        model = get_peft_model(model, LORA_CONFIG)\n",
                "else:\n",
                "    print(\"üÜï Starting with fresh LoRA adapters\")\n",
                "    model = get_peft_model(model, LORA_CONFIG)\n",
                "\n",
                "model.print_trainable_parameters()\n",
                "print(\"Model ready!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 5: Load CSV"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "df = pd.read_csv(\"/content/chest_x_ray_images_labels_sample.csv\")\n",
                "print(f\"Loaded {len(df)} samples\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 6: Upload Images"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from google.colab import files\n",
                "os.makedirs(\"/content/images\", exist_ok=True)\n",
                "print(\"Upload X-ray images:\")\n",
                "for name, content in files.upload().items():\n",
                "    open(f\"/content/images/{name}\", \"wb\").write(content)\n",
                "print(\"Done\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 7: Create Dataset (SIMPLIFIED)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from PIL import Image\n",
                "from torch.utils.data import Dataset, random_split\n",
                "from pathlib import Path\n",
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "class XrayDataset(Dataset):\n",
                "    def __init__(self, df, img_dir, proc):\n",
                "        self.df = df.dropna(subset=[\"ImageID\", \"Report\"]).reset_index(drop=True)\n",
                "        self.img_dir = Path(img_dir)\n",
                "        self.proc = proc\n",
                "        # Filter to only samples with images that exist\n",
                "        valid_rows = []\n",
                "        for i, row in self.df.iterrows():\n",
                "            img_id = str(row[\"ImageID\"])\n",
                "            p = self.img_dir / img_id\n",
                "            if not p.exists():\n",
                "                matches = list(self.img_dir.glob(f\"{img_id.split('.')[0]}.*\"))\n",
                "                if matches:\n",
                "                    valid_rows.append(i)\n",
                "            else:\n",
                "                valid_rows.append(i)\n",
                "        self.df = self.df.iloc[valid_rows].reset_index(drop=True)\n",
                "        print(f\"Valid samples with images: {len(self.df)}\")\n",
                "        \n",
                "    def __len__(self): return len(self.df)\n",
                "    \n",
                "    def __getitem__(self, i):\n",
                "        row = self.df.iloc[i]\n",
                "        img_id = str(row[\"ImageID\"])\n",
                "        p = self.img_dir / img_id\n",
                "        if not p.exists():\n",
                "            matches = list(self.img_dir.glob(f\"{img_id.split('.')[0]}.*\"))\n",
                "            p = matches[0]\n",
                "        img = Image.open(p).convert(\"RGB\")\n",
                "        \n",
                "        # Simple prompt + answer format\n",
                "        prompt = \"describe this chest xray:\"\n",
                "        answer = str(row[\"Report\"]).strip()\n",
                "        \n",
                "        # Process with suffix (the answer we want to predict)\n",
                "        model_inputs = self.proc(\n",
                "            text=prompt,\n",
                "            images=img,\n",
                "            suffix=answer,\n",
                "            return_tensors=\"pt\",\n",
                "            padding=\"max_length\",\n",
                "            truncation=True,\n",
                "            max_length=512,\n",
                "        )\n",
                "        \n",
                "        return {k: v.squeeze(0) for k, v in model_inputs.items()}\n",
                "\n",
                "ds = XrayDataset(df, \"/content/images\", processor)\n",
                "if len(ds) > 1:\n",
                "    train_size = max(1, int(0.9 * len(ds)))\n",
                "    val_size = len(ds) - train_size\n",
                "    train_ds, val_ds = random_split(ds, [train_size, val_size])\n",
                "else:\n",
                "    train_ds = val_ds = ds\n",
                "print(f\"Train: {len(train_ds)}, Val: {len(val_ds)}\")\n",
                "\n",
                "# Test one sample\n",
                "sample = ds[0]\n",
                "print(f\"Sample keys: {sample.keys()}\")\n",
                "print(f\"input_ids shape: {sample['input_ids'].shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 8: Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from dataclasses import dataclass\n",
                "from typing import Dict, List\n",
                "from transformers import Trainer, TrainingArguments\n",
                "import os\n",
                "\n",
                "@dataclass\n",
                "class Collator:\n",
                "    def __call__(self, f: List[Dict]) -> Dict[str, torch.Tensor]:\n",
                "        return {k: torch.stack([x[k] for x in f]) for k in f[0]}\n",
                "\n",
                "# ========== RESUME FROM CHECKPOINT CONFIG (with fallback) ==========\n",
                "CHECKPOINT_PRIMARY = \"/content/drive/MyDrive/ExplainMyXray_Models/interrupted_checkpoint\"\n",
                "CHECKPOINT_FALLBACK = \"/content/drive/MyDrive/medgemma_advanced_lora/checkpoint-250\"\n",
                "\n",
                "# Try primary first, then fallback\n",
                "if os.path.exists(CHECKPOINT_PRIMARY):\n",
                "    RESUME_CHECKPOINT = CHECKPOINT_PRIMARY\n",
                "    print(f\"‚úÖ Will resume from primary checkpoint: {RESUME_CHECKPOINT}\")\n",
                "elif os.path.exists(CHECKPOINT_FALLBACK):\n",
                "    RESUME_CHECKPOINT = CHECKPOINT_FALLBACK\n",
                "    print(f\"‚úÖ Will resume from fallback checkpoint: {RESUME_CHECKPOINT}\")\n",
                "else:\n",
                "    RESUME_CHECKPOINT = None\n",
                "    print(\"‚ö†Ô∏è No checkpoint found - starting fresh training\")\n",
                "# ====================================================================\n",
                "\n",
                "args = TrainingArguments(\n",
                "    output_dir=OUTPUT_DIR,\n",
                "    per_device_train_batch_size=BATCH_SIZE,\n",
                "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
                "    num_train_epochs=NUM_EPOCHS,\n",
                "    learning_rate=LEARNING_RATE,\n",
                "    bf16=True,\n",
                "    save_strategy=\"steps\",\n",
                "    save_steps=100,  # Save more frequently to avoid losing progress\n",
                "    logging_steps=10,\n",
                "    eval_strategy=\"steps\",\n",
                "    eval_steps=100,\n",
                "    load_best_model_at_end=True,\n",
                "    report_to=\"none\",\n",
                "    dataloader_pin_memory=False,\n",
                "    gradient_checkpointing=True,\n",
                "    remove_unused_columns=False,\n",
                ")\n",
                "\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=args,\n",
                "    train_dataset=train_ds,\n",
                "    eval_dataset=val_ds,\n",
                "    data_collator=Collator(),\n",
                ")\n",
                "\n",
                "trainer.train(resume_from_checkpoint=RESUME_CHECKPOINT)\n",
                "print(\"Training...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 9: Save & Download"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.save_pretrained(OUTPUT_DIR)\n",
                "processor.save_pretrained(OUTPUT_DIR)\n",
                "import shutil\n",
                "from google.colab import files\n",
                "shutil.make_archive(\"medgemma_lora\", \"zip\", OUTPUT_DIR)\n",
                "files.download(\"medgemma_lora.zip\")\n",
                "print(\"Downloaded!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 10: Test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from PIL import Image\n",
                "from pathlib import Path\n",
                "imgs = list(Path(\"/content/images\").glob(\"*.*\"))\n",
                "if imgs:\n",
                "    img = Image.open(imgs[0]).convert(\"RGB\")\n",
                "    inp = processor(images=img, text=\"describe this chest xray:\", return_tensors=\"pt\").to(\"cuda\")\n",
                "    with torch.no_grad():\n",
                "        out = model.generate(**inp, max_new_tokens=50, do_sample=True)\n",
                "    print(processor.decode(out[0], skip_special_tokens=True))"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
