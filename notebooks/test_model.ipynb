{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf628b03",
   "metadata": {},
   "source": [
    "# üî¨ ExplainMyXray - Model Testing Notebook\n",
    "\n",
    "Test your fine-tuned PaliGemma model on chest X-ray images.\n",
    "\n",
    "**Checkpoints Available:**\n",
    "- `interrupted_checkpoint` - Latest training progress\n",
    "- `checkpoint-250` - Step 250 checkpoint\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88334ead",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2132c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate bitsandbytes peft pillow matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d859da",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Mount Google Drive & Setup Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30ff269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "\n",
    "# Checkpoint paths\n",
    "DRIVE_CHECKPOINT = \"/content/drive/MyDrive/ExplainMyXray_Models/interrupted_checkpoint\"\n",
    "CHECKPOINT_250 = \"/content/drive/MyDrive/ExplainMyXray_Models/checkpoint-250\"  # If you saved it\n",
    "\n",
    "# Check what's available\n",
    "print(\"üìÅ Checking available checkpoints...\")\n",
    "if os.path.exists(DRIVE_CHECKPOINT):\n",
    "    print(f\"‚úÖ interrupted_checkpoint found\")\n",
    "    files = os.listdir(DRIVE_CHECKPOINT)\n",
    "    print(f\"   Files: {files[:5]}...\" if len(files) > 5 else f\"   Files: {files}\")\n",
    "else:\n",
    "    print(f\"‚ùå interrupted_checkpoint not found\")\n",
    "\n",
    "if os.path.exists(CHECKPOINT_250):\n",
    "    print(f\"‚úÖ checkpoint-250 found\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è checkpoint-250 not found on Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb860f97",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ HuggingFace Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21270c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# ‚ö†Ô∏è DO NOT hardcode your token here!\n",
    "# Set HF_TOKEN in .env file or run: huggingface-cli login\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\", \"\")\n",
    "if HF_TOKEN:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"‚úÖ Logged in via HF_TOKEN environment variable\")\n",
    "else:\n",
    "    login()  # Interactive login\n",
    "    print(\"‚úÖ HuggingFace authentication successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012c3a21",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Load Model & Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2660b92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, PaliGemmaForConditionalGeneration, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# Base model name\n",
    "BASE_MODEL = \"google/paligemma-3b-pt-224\"\n",
    "\n",
    "# Choose which checkpoint to load\n",
    "CHECKPOINT_PATH = DRIVE_CHECKPOINT  # Change to CHECKPOINT_250 if needed\n",
    "\n",
    "print(f\"üì• Loading from: {CHECKPOINT_PATH}\")\n",
    "\n",
    "# Quantization config (same as training)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load processor\n",
    "print(\"Loading processor...\")\n",
    "processor = AutoProcessor.from_pretrained(BASE_MODEL, token=HF_TOKEN)\n",
    "\n",
    "# Load base model\n",
    "print(\"Loading base model (4-bit quantized)...\")\n",
    "base_model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    token=HF_TOKEN,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Load LoRA adapter\n",
    "print(\"Loading LoRA adapter...\")\n",
    "model = PeftModel.from_pretrained(base_model, CHECKPOINT_PATH)\n",
    "model.eval()\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"   Device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a714a34d",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Define Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1e09ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def explain_xray(image_path, prompt=\"Explain this chest X-ray:\", max_new_tokens=256):\n",
    "    \"\"\"\n",
    "    Generate explanation for a chest X-ray image.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the X-ray image\n",
    "        prompt: Text prompt for the model\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "    \n",
    "    Returns:\n",
    "        Generated explanation text\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Prepare inputs\n",
    "    inputs = processor(\n",
    "        text=prompt,\n",
    "        images=image,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    generated_text = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Remove prompt from output if present\n",
    "    if prompt in generated_text:\n",
    "        generated_text = generated_text.split(prompt)[-1].strip()\n",
    "    \n",
    "    return generated_text, image\n",
    "\n",
    "\n",
    "def display_result(image_path, prompt=\"Explain this chest X-ray:\"):\n",
    "    \"\"\"Display image and its generated explanation.\"\"\"\n",
    "    explanation, image = explain_xray(image_path, prompt)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    ax.imshow(image, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(\"Chest X-Ray\", fontsize=14)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üî¨ MODEL EXPLANATION:\")\n",
    "    print(\"=\"*60)\n",
    "    print(explanation)\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return explanation\n",
    "\n",
    "print(\"‚úÖ Inference functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9202b1c",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Download Test Images (Kaggle Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b30d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Setup Kaggle credentials\n",
    "os.makedirs('/root/.kaggle', exist_ok=True)\n",
    "\n",
    "kaggle_creds = {\n",
    "    \"username\": \"your_kaggle_username\",  # Replace with your username\n",
    "    \"key\": \"your_kaggle_key\"  # Replace with your key\n",
    "}\n",
    "\n",
    "# Or use the userdata approach if you have secrets set up\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    kaggle_creds = {\n",
    "        \"username\": userdata.get('KAGGLE_USERNAME'),\n",
    "        \"key\": userdata.get('KAGGLE_KEY')\n",
    "    }\n",
    "except:\n",
    "    pass\n",
    "\n",
    "with open('/root/.kaggle/kaggle.json', 'w') as f:\n",
    "    json.dump(kaggle_creds, f)\n",
    "os.chmod('/root/.kaggle/kaggle.json', 0o600)\n",
    "\n",
    "# Download a small subset for testing\n",
    "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia -p /content/test_data --unzip\n",
    "\n",
    "print(\"\\n‚úÖ Test data downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a24f57",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Test on Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96124581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "\n",
    "# Find test images\n",
    "test_dir = \"/content/test_data/chest_xray/test\"\n",
    "\n",
    "normal_images = glob.glob(f\"{test_dir}/NORMAL/*.jpeg\")\n",
    "pneumonia_images = glob.glob(f\"{test_dir}/PNEUMONIA/*.jpeg\")\n",
    "\n",
    "print(f\"Found {len(normal_images)} normal images\")\n",
    "print(f\"Found {len(pneumonia_images)} pneumonia images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0258802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a NORMAL image\n",
    "print(\"\\n\" + \"üü¢\"*30)\n",
    "print(\"TESTING ON NORMAL X-RAY\")\n",
    "print(\"üü¢\"*30)\n",
    "\n",
    "if normal_images:\n",
    "    test_normal = random.choice(normal_images)\n",
    "    print(f\"\\nImage: {os.path.basename(test_normal)}\")\n",
    "    explanation = display_result(test_normal)\n",
    "else:\n",
    "    print(\"No normal images found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01293ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a PNEUMONIA image\n",
    "print(\"\\n\" + \"üî¥\"*30)\n",
    "print(\"TESTING ON PNEUMONIA X-RAY\")\n",
    "print(\"üî¥\"*30)\n",
    "\n",
    "if pneumonia_images:\n",
    "    test_pneumonia = random.choice(pneumonia_images)\n",
    "    print(f\"\\nImage: {os.path.basename(test_pneumonia)}\")\n",
    "    explanation = display_result(test_pneumonia)\n",
    "else:\n",
    "    print(\"No pneumonia images found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f438d8",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Test with Custom Image (Upload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19edf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"üì§ Upload a chest X-ray image to test:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "for filename in uploaded.keys():\n",
    "    print(f\"\\nTesting: {filename}\")\n",
    "    explanation = display_result(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d63e1f4",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Batch Testing & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f269fe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_test(image_paths, label=\"Test\", num_samples=5):\n",
    "    \"\"\"\n",
    "    Test multiple images and collect results.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    samples = random.sample(image_paths, min(num_samples, len(image_paths)))\n",
    "    \n",
    "    for i, img_path in enumerate(samples, 1):\n",
    "        print(f\"\\n[{i}/{len(samples)}] Processing: {os.path.basename(img_path)}\")\n",
    "        try:\n",
    "            explanation, _ = explain_xray(img_path)\n",
    "            results.append({\n",
    "                \"image\": os.path.basename(img_path),\n",
    "                \"label\": label,\n",
    "                \"explanation\": explanation\n",
    "            })\n",
    "            print(f\"   ‚úÖ Generated {len(explanation.split())} words\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error: {e}\")\n",
    "            results.append({\n",
    "                \"image\": os.path.basename(img_path),\n",
    "                \"label\": label,\n",
    "                \"explanation\": f\"ERROR: {e}\"\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run batch test\n",
    "print(\"üî¨ Running batch evaluation...\\n\")\n",
    "normal_results = batch_test(normal_images, label=\"NORMAL\", num_samples=3)\n",
    "pneumonia_results = batch_test(pneumonia_images, label=\"PNEUMONIA\", num_samples=3)\n",
    "\n",
    "all_results = normal_results + pneumonia_results\n",
    "print(f\"\\n\\n‚úÖ Batch test complete! Processed {len(all_results)} images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f1fff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all results\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(all_results)\n",
    "df['explanation_length'] = df['explanation'].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(\"\\nüìä BATCH TEST RESULTS:\")\n",
    "print(\"=\"*80)\n",
    "for _, row in df.iterrows():\n",
    "    print(f\"\\nüñºÔ∏è  Image: {row['image']}\")\n",
    "    print(f\"üè∑Ô∏è  Label: {row['label']}\")\n",
    "    print(f\"üìù Explanation ({row['explanation_length']} words):\")\n",
    "    print(f\"   {row['explanation'][:300]}...\" if len(row['explanation']) > 300 else f\"   {row['explanation']}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6032b1",
   "metadata": {},
   "source": [
    "## üîü Compare Different Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7522944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different prompts on the same image\n",
    "test_image = random.choice(pneumonia_images) if pneumonia_images else random.choice(normal_images)\n",
    "\n",
    "prompts = [\n",
    "    \"Explain this chest X-ray:\",\n",
    "    \"Describe the findings in this chest X-ray:\",\n",
    "    \"What abnormalities are visible in this X-ray?\",\n",
    "    \"Provide a radiological report for this chest X-ray:\",\n",
    "    \"Is this X-ray normal or abnormal? Explain:\"\n",
    "]\n",
    "\n",
    "print(f\"üñºÔ∏è Testing image: {os.path.basename(test_image)}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display the image once\n",
    "img = Image.open(test_image)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title(\"Test Image\")\n",
    "plt.show()\n",
    "\n",
    "# Test each prompt\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nüìù Prompt: \\\"{prompt}\\\"\")\n",
    "    print(\"-\"*60)\n",
    "    explanation, _ = explain_xray(test_image, prompt=prompt)\n",
    "    print(f\"Response: {explanation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7896ba9",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Summary\n",
    "\n",
    "This notebook allows you to:\n",
    "1. ‚úÖ Load your fine-tuned PaliGemma model\n",
    "2. ‚úÖ Test on Kaggle chest X-ray dataset\n",
    "3. ‚úÖ Upload and test custom images\n",
    "4. ‚úÖ Batch evaluate multiple images\n",
    "5. ‚úÖ Compare different prompts\n",
    "\n",
    "**Next Steps:**\n",
    "- Continue training if results are not satisfactory\n",
    "- Try different prompts for better explanations\n",
    "- Export model for deployment"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
